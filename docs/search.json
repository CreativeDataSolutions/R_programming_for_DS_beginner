[
  {
    "objectID": "R_studio.html",
    "href": "R_studio.html",
    "title": "1  Rstudio",
    "section": "",
    "text": "DALL·E 2023-04-12 20.55.50 - hacker unicorn writing R code using the Rstudio IDE, cyberpunk\n\n\nRstudio is one the most comprehensive free interactive development environments (IDE) for R.\nThe following reference covers he major steps to install and configure your R studio.\nMajor steps include:\n\ninstall the latest version of R\ninstall Rstudio\nconfigure Rstudio defaults and appearance\n\nOnce you have Rstudio installed you can select Tools >> Global options and configure your preferences e.g. apperance."
  },
  {
    "objectID": "openapi.html",
    "href": "openapi.html",
    "title": "7  AI coding help",
    "section": "",
    "text": "There is a lot of hype around tools like chatGPT which is a class of large language models (LLM's) for usingnatural language inputs to answer questions, including how to write code."
  },
  {
    "objectID": "openapi.html#install-chatgpt-rstudio-add-in",
    "href": "openapi.html#install-chatgpt-rstudio-add-in",
    "title": "7  AI coding help",
    "section": "7.1 Install chatGPT Rstudio add in",
    "text": "7.1 Install chatGPT Rstudio add in\n\n7.1.1 Prerequisites\n\nMake an OpenAI account.\nCreate an OpenAI API key to use with the package.\nSet the API key up in Rstudio\n\n\n# install.packages(c(\"gptstudio\",\"waiter\"))\nlibrary(gptstudio)"
  },
  {
    "objectID": "openapi.html#chatgpt-rstudio-add-in",
    "href": "openapi.html#chatgpt-rstudio-add-in",
    "title": "7  AI coding help",
    "section": "7.2 chatGPT Rstudio add in",
    "text": "7.2 chatGPT Rstudio add in\n\nTry to ask the openAI LLM how it would code up some of the examples you have seen so far. Try running the code, giving feedback about any errors to the chatbot and see if you can customize and/or improve the code you have seen so far."
  },
  {
    "objectID": "openapi.html#appendix",
    "href": "openapi.html#appendix",
    "title": "7  AI coding help",
    "section": "7.3 Appendix",
    "text": "7.3 Appendix\n\ngptstudio\nOpenAI"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R programming for Data Science - A Beginner’s Guide",
    "section": "",
    "text": "This the following course covers the major topics for beginners to get started with R programming for data analysis and visualization.\n\n\n\nHow to get up and running with R using the Rstudio\nUnderstand R data types and functions\nUse data frame manipulation to wrangle data\nVisualize and plot with ggplot2 and plotly\nExploratory Data Analysis\nPut it all together by creating a reproducible data analysis reports\nUse AI tools to improve your code\n\n\n\n\n\nR >= 4.2.1\nRStudio >= 2022.12.0 Build 353\n\nSee section on Rsudio to get more details."
  },
  {
    "objectID": "data_wrangling.html#data",
    "href": "data_wrangling.html#data",
    "title": "3  Data Wrangling",
    "section": "3.1 Data",
    "text": "3.1 Data\nThe best way to learn how to manipulate data in R is to find a simple data set and practice different transformations. The datastes R package has many examples to experiment with.\nWhile we will work with demo data for the rest of the examples, in practice you will likely want to import your own custom data sets.\nTake a look at some available data sets in the datastes R package.\n\nif(!require(datasets)) install.packages('datasets') #install package for the first time\nlibrary(datasets) # load package in session\n\n.data<-data()\n# str(.data) #take a look at the resulting object \nhead(.data$results[,c('Item','Title')]) #extract specific elements\n\n     Item                    \n[1,] \"billboard\"             \n[2,] \"cms_patient_care\"      \n[3,] \"cms_patient_experience\"\n[4,] \"construction\"          \n[5,] \"fish_encounters\"       \n[6,] \"household\"             \n     Title                                                   \n[1,] \"Song rankings for Billboard top 100 in the year 2000\"  \n[2,] \"Data from the Centers for Medicare & Medicaid Services\"\n[3,] \"Data from the Centers for Medicare & Medicaid Services\"\n[4,] \"Completed construction in the US in 2018\"              \n[5,] \"Fish encounters\"                                       \n[6,] \"Household data\"                                        \n\n\nLets find some data about cars.\n\n#lets look for a key word (i.e. substring) in Title of the datasets\nkeyword<-'car'\n#try ?grepl to see how else it can be used\ncars_id<-grepl(keyword,as.data.frame(.data$result)$Title,ignore.case = T)\n.data$results[cars_id,]\n\n     Package    LibPath                                         \n[1,] \"tidyr\"    \"C:/Users/think/AppData/Local/R/win-library/4.2\"\n[2,] \"tidyr\"    \"C:/Users/think/AppData/Local/R/win-library/4.2\"\n[3,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n[4,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n[5,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n     Item                    \n[1,] \"cms_patient_care\"      \n[2,] \"cms_patient_experience\"\n[3,] \"CO2\"                   \n[4,] \"cars\"                  \n[5,] \"mtcars\"                \n     Title                                                   \n[1,] \"Data from the Centers for Medicare & Medicaid Services\"\n[2,] \"Data from the Centers for Medicare & Medicaid Services\"\n[3,] \"Carbon Dioxide Uptake in Grass Plants\"                 \n[4,] \"Speed and Stopping Distances of Cars\"                  \n[5,] \"Motor Trend Car Road Tests\"                            \n\n\nSee here for more examples of string processing in R.\nLets load and review the mtcars data set.\n\ndata(\"mtcars\")\n# View(mtcars) #table view for small data - uncomment this line to see an interactive table\n#note we can also look at this data in the environment tab of Rstudio\n\nSummarize the data.\n\nsummary(mtcars) # see more advanced data summaries in the `Exploratory Data Analysis` section\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nNext lets introduce a more readable way to link R functions. We will use the pipe operator %>%.\n\n#lets format the meant miles per gallon to two digits\nround(mean(mtcars$mpg),2)\n\n[1] 20.09\n\n# we can rewrite this as a pipe where x %>% f(.) is equivalent to f(x). The '.' can be implicit or used to denote the 'x' on the left side of the equation.\nmtcars$mpg %>% \n  mean(.) %>%\n  round(.,2)\n\n[1] 20.09\n\n\nNote %>% can be imported in different ways and depends on the magrittr library. A more recent R update now allows to call the pipe operator from the base library as |>.\nWe can use the R package dplyr to create a custom summary. Lets calculate the mean and standard deviation of each column.\n\nmy_summary<-mtcars %>%\n  summarise_each(., funs(mean,stdev=sd))\n\nmy_summary\n\n  mpg_mean cyl_mean disp_mean  hp_mean drat_mean wt_mean qsec_mean vs_mean\n1 20.09062   6.1875  230.7219 146.6875  3.596563 3.21725  17.84875  0.4375\n  am_mean gear_mean carb_mean mpg_stdev cyl_stdev disp_stdev hp_stdev\n1 0.40625    3.6875    2.8125  6.026948  1.785922   123.9387 68.56287\n  drat_stdev  wt_stdev qsec_stdev  vs_stdev  am_stdev gear_stdev carb_stdev\n1  0.5346787 0.9784574   1.786943 0.5040161 0.4989909  0.7378041     1.6152\n\n\nA better format could be to output the results as columns for each row which correspond to the original columns in the data. We can check the dplyrcheat-sheet to see what other data wrangling operations this package enables.\n\nmeans <- my_summary %>%\n  select(ends_with('_mean')) %>%\n  t()\n  \nstdevs <- my_summary %>%\n  select(!ends_with('_mean')) %>%\n t()\n\n(my_summary2 <-\n    data.frame(\n      variable = colnames(mtcars),\n      mean = means,\n      stdev = stdevs\n    ))\n\n          variable       mean       stdev\nmpg_mean       mpg  20.090625   6.0269481\ncyl_mean       cyl   6.187500   1.7859216\ndisp_mean     disp 230.721875 123.9386938\nhp_mean         hp 146.687500  68.5628685\ndrat_mean     drat   3.596563   0.5346787\nwt_mean         wt   3.217250   0.9784574\nqsec_mean     qsec  17.848750   1.7869432\nvs_mean         vs   0.437500   0.5040161\nam_mean         am   0.406250   0.4989909\ngear_mean     gear   3.687500   0.7378041\ncarb_mean     carb   2.812500   1.6152000\n\n\nIn addition to dplyr the tidyr package also offers many useful data manipulation functions.\ndplyr\n\ntidyr\n\nLets round the results and then create a summary as mean +/- stdev . To do this we will create our first function. A function simply executes (calls) on a set of inputs (arguments).\n\n# lets start with the logic and then convert it to a function\n\n#inputs\ndigits<-1\nx<-my_summary2[,2,drop=FALSE] # data to test with-- second column\n\n#step 1 - round\nx %>%\n round(.,digits)\n\n           mean\nmpg_mean   20.1\ncyl_mean    6.2\ndisp_mean 230.7\nhp_mean   146.7\ndrat_mean   3.6\nwt_mean     3.2\nqsec_mean  17.8\nvs_mean     0.4\nam_mean     0.4\ngear_mean   3.7\ncarb_mean   2.8\n\n#step 2 - combine two columns\nmy_summary2 %>%\n  select(one_of(c('mean','stdev'))) %>%\n  unite(.,'mean_sd',sep= \" +/- \")\n\n                                  mean_sd\nmpg_mean    20.090625 +/- 6.0269480520891\ncyl_mean      6.1875 +/- 1.78592164694654\ndisp_mean 230.721875 +/- 123.938693831382\nhp_mean     146.6875 +/- 68.5628684893206\ndrat_mean 3.5965625 +/- 0.534678736070971\nwt_mean     3.21725 +/- 0.978457442989697\nqsec_mean   17.84875 +/- 1.78694323609684\nvs_mean      0.4375 +/- 0.504016128774185\nam_mean     0.40625 +/- 0.498990917235846\ngear_mean    3.6875 +/- 0.737804065256947\ncarb_mean     2.8125 +/- 1.61519997763185\n\n#create a function to do both at the same time on arbitrary inputs\n#note we are using Roxygen syntax (ctrl+shift+alt+R) to also document our funtion which is relevant when making R packages\n\n#' summary_function\n#'\n#' @param x data.frame\n#' @param digits int, number of digits to round to\n#' @param name str, colum name of results\n#' @param sep str, what to separate the combined columns with\n#'\n#' @return data.frame where each column is rounded to `digits` and combined into a string collapsed on `sep`.\n#' @export\n#' @details Round each column to `digits` and combined all columns into a string collapsed on `sep`.\n#' @examples\nsummary_function <-\n  function(x,\n           digits,\n           name = 'mean_sd',\n           sep = ' +/- ') {\n    x %>%\n      summarise(across(), round(.,digits)) %>%\n      unite(.,col= !!name, sep = sep) # ... use !! or {{}} for string arguments passed to dplyr verbs read more: https://dplyr.tidyverse.org/articles/programming.html\n  }\n\n#call function\n(tmp <-\n    my_summary2 %>%\n    select(one_of(c('mean', 'stdev'))) %>%\n    summary_function(., digits=2)\n  )\n\n             mean_sd\n1     20.09 +/- 6.03\n2      6.19 +/- 1.79\n3  230.72 +/- 123.94\n4   146.69 +/- 68.56\n5       3.6 +/- 0.53\n6      3.22 +/- 0.98\n7     17.85 +/- 1.79\n8       0.44 +/- 0.5\n9       0.41 +/- 0.5\n10     3.69 +/- 0.74\n11     2.81 +/- 1.62\n\n#add created object to our data\n(my_summary2<- my_summary2  %>%\n  cbind(.,tmp)\n  )\n\n          variable       mean       stdev           mean_sd\nmpg_mean       mpg  20.090625   6.0269481    20.09 +/- 6.03\ncyl_mean       cyl   6.187500   1.7859216     6.19 +/- 1.79\ndisp_mean     disp 230.721875 123.9386938 230.72 +/- 123.94\nhp_mean         hp 146.687500  68.5628685  146.69 +/- 68.56\ndrat_mean     drat   3.596563   0.5346787      3.6 +/- 0.53\nwt_mean         wt   3.217250   0.9784574     3.22 +/- 0.98\nqsec_mean     qsec  17.848750   1.7869432    17.85 +/- 1.79\nvs_mean         vs   0.437500   0.5040161      0.44 +/- 0.5\nam_mean         am   0.406250   0.4989909      0.41 +/- 0.5\ngear_mean     gear   3.687500   0.7378041     3.69 +/- 0.74\ncarb_mean     carb   2.812500   1.6152000     2.81 +/- 1.62\n\n\nNote, it can also be useful to call a functions by their string names using do.call('function_name',list(<arguments>))."
  },
  {
    "objectID": "data_wrangling.html#loops",
    "href": "data_wrangling.html#loops",
    "title": "3  Data Wrangling",
    "section": "3.2 Loops",
    "text": "3.2 Loops\nWhen we executed a function over each column this is executed by looping the calculation n number of times where n is equal to the number of columns. While modern libraries like dplyr, tidyr and purrr do this internally, it is useful to understand how to create your own loops. The simplest way to loop is using the for function. Note R is a vectorized language and looping is often discouraged because its much slower; this approach is still very useful for prototyping complex and simpler to read, understand and debug code.\nLets use apply to mimic summarise.\n\nmeans<-apply(mtcars,2,mean) # the margin 1 == across rows or 2 == columns\nstdevs<-apply(mtcars,2,'sd') # functions or their names are supported\n\ndata.frame(variable=colnames(mtcars),mean=means,stdev=stdevs)\n\n     variable       mean       stdev\nmpg       mpg  20.090625   6.0269481\ncyl       cyl   6.187500   1.7859216\ndisp     disp 230.721875 123.9386938\nhp         hp 146.687500  68.5628685\ndrat     drat   3.596563   0.5346787\nwt         wt   3.217250   0.9784574\nqsec     qsec  17.848750   1.7869432\nvs         vs   0.437500   0.5040161\nam         am   0.406250   0.4989909\ngear     gear   3.687500   0.7378041\ncarb     carb   2.812500   1.6152000\n\n\nNext, lets repeat our column summary calculation using a for loop.\n\nresults<-list() #initialize an empty list to store results in. Note it is more efficient to make a list of the same length as the number of elements you want to store.\nfor (i in 1:ncol(mtcars)){\n  \n  results$mean[i]<-mtcars[,i] %>%\n    mean() # store results in position [i] in the list results element named 'mean'\n  results$sd[i]<-mtcars[,i] %>%\n    sd()\n}\n\ndata.frame(variable=colnames(mtcars),results)\n\n   variable       mean          sd\n1       mpg  20.090625   6.0269481\n2       cyl   6.187500   1.7859216\n3      disp 230.721875 123.9386938\n4        hp 146.687500  68.5628685\n5      drat   3.596563   0.5346787\n6        wt   3.217250   0.9784574\n7      qsec  17.848750   1.7869432\n8        vs   0.437500   0.5040161\n9        am   0.406250   0.4989909\n10     gear   3.687500   0.7378041\n11     carb   2.812500   1.6152000\n\n\nA lapply is more convenient and versatile version of a for loop.\n\nlapply(mtcars,function(x){\n  c(mean=mean(x),sd=sd(x))\n}) %>%\n  do.call('rbind',.) %>% #combine list elements rowwise; use 'cbind' to combine columnwise \n  data.frame(variable=colnames(mtcars),.) \n\n     variable       mean          sd\nmpg       mpg  20.090625   6.0269481\ncyl       cyl   6.187500   1.7859216\ndisp     disp 230.721875 123.9386938\nhp         hp 146.687500  68.5628685\ndrat     drat   3.596563   0.5346787\nwt         wt   3.217250   0.9784574\nqsec     qsec  17.848750   1.7869432\nvs         vs   0.437500   0.5040161\nam         am   0.406250   0.4989909\ngear     gear   3.687500   0.7378041\ncarb     carb   2.812500   1.6152000\n\n\nNext, we will build on our function to create summaries for groups of rows. Lets summarize miles per gallon mpg for cars with different number of cylinders cyl. First lets create the functions for the individual steps.\n\n#we need to regenerate our our original analysis\n#we can take this opportunity to functionalize all the steps\n#1) calculate mean and standard deviation of each column\n#2) pivot data\n#3) create a custom summary\n\n#1 - execute function(s) on each column\ncolumn_summary<-function(data,functions = c(mean=mean,stdev=sd)){\n  data %>%\n  summarise_each(., funs(!!!(functions))) # use !!! for functions or unquoted arguments\n  \n}\n\n#test #1 \n(x<-mtcars %>% \n  column_summary()\n)\n\n  mpg_mean cyl_mean disp_mean  hp_mean drat_mean wt_mean qsec_mean vs_mean\n1 20.09062   6.1875  230.7219 146.6875  3.596563 3.21725  17.84875  0.4375\n  am_mean gear_mean carb_mean mpg_stdev cyl_stdev disp_stdev hp_stdev\n1 0.40625    3.6875    2.8125  6.026948  1.785922   123.9387 68.56287\n  drat_stdev  wt_stdev qsec_stdev  vs_stdev  am_stdev gear_stdev carb_stdev\n1  0.5346787 0.9784574   1.786943 0.5040161 0.4989909  0.7378041     1.6152\n\n#2 format results\n#we can explicitly pass column names we want to separate or infer based on suffix\n#2 A. infer common suffix\nget_unique_suffix<-function(data,sep='_'){\n  tmp<-colnames(data) %>%  #get column names\n  strsplit(.,'_') %>% #split string on '_''\n  do.call('rbind',.) %>% # combine list elements row wise\n  .[,2] %>% #get second column, better to reference by name\n  unique() #get unique values\n}\n\n#test 2 A\nget_unique_suffix(x)\n\n#2 B transpose elements\ntranspose_on_suffix<-function(data,sep='_'){\n  \n  suffixes<- data %>%\n    get_unique_suffix(.,sep)\n  \n  #loop over suffixes and transpose\n  lapply(suffixes,function(x){\n    data %>%\n    select(ends_with(x)) %>%\n    t() # transpose operation, i.e. rotate rows to columns\n    \n  }) %>%\n  do.call('cbind',.) %>% # bind list elements columnwise\n    data.frame() %>% # make sure its a data.frame\n    setNames(.,suffixes) #set column names\n}\n\n#test 2 A\ntranspose_on_suffix(x)\n\n                mean       stdev\nmpg_mean   20.090625   6.0269481\ncyl_mean    6.187500   1.7859216\ndisp_mean 230.721875 123.9386938\nhp_mean   146.687500  68.5628685\ndrat_mean   3.596563   0.5346787\nwt_mean     3.217250   0.9784574\nqsec_mean  17.848750   1.7869432\nvs_mean     0.437500   0.5040161\nam_mean     0.406250   0.4989909\ngear_mean   3.687500   0.7378041\ncarb_mean   2.812500   1.6152000\n\n\nNext we will execute our workflow grouping by different number of cylinders cyl.\n\n#next lets use our first factor to group our data\nclass(mtcars$cyl) #we want to convert this class to a factor\n\n[1] \"numeric\"\n\n#factors are a categorical vectors which are used for grouping operations\nstr(as.factor(mtcars$cyl))\n\n Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n\n#we could A) create a custom loop or B) modify our origin to handle a grouping variable\n\n#A) \n#we will split the data into list elements for each group and execute our simple workflow \ndata<-mtcars # make this more generic\ntmp<-data %>%\n  mutate(groups=as.factor(cyl)) #note, we need to save to an intermediate object for split to play nice with dplyr\n\ntmp %>% \n  split(.,.$groups) %>%\n  lapply(.,function(x){\n    x %>% select(-groups) %>% #remove factor which will cause an issue -- native dplyr handles this for us\n      column_summary(.) %>%\n      transpose_on_suffix(.) %>%\n      mutate(groups=x$groups %>% unique(),variable=colnames(data)) #note, we lost the variable names during the calculation Some options to fix this are A) save and carry forward variables in the original calculation (best -- complicated) or B) set variables as our data column names (simple but hard for others to understand and verify as correct)\n  }) %>%\n  do.call('rbind',.)\n\n                   mean      stdev groups variable\n4.mpg_mean   26.6636364  4.5098277      4      mpg\n4.cyl_mean    4.0000000  0.0000000      4      cyl\n4.disp_mean 105.1363636 26.8715937      4     disp\n4.hp_mean    82.6363636 20.9345300      4       hp\n4.drat_mean   4.0709091  0.3654711      4     drat\n4.wt_mean     2.2857273  0.5695637      4       wt\n4.qsec_mean  19.1372727  1.6824452      4     qsec\n4.vs_mean     0.9090909  0.3015113      4       vs\n4.am_mean     0.7272727  0.4670994      4       am\n4.gear_mean   4.0909091  0.5393599      4     gear\n4.carb_mean   1.5454545  0.5222330      4     carb\n6.mpg_mean   19.7428571  1.4535670      6      mpg\n6.cyl_mean    6.0000000  0.0000000      6      cyl\n6.disp_mean 183.3142857 41.5624602      6     disp\n6.hp_mean   122.2857143 24.2604911      6       hp\n6.drat_mean   3.5857143  0.4760552      6     drat\n6.wt_mean     3.1171429  0.3563455      6       wt\n6.qsec_mean  17.9771429  1.7068657      6     qsec\n6.vs_mean     0.5714286  0.5345225      6       vs\n6.am_mean     0.4285714  0.5345225      6       am\n6.gear_mean   3.8571429  0.6900656      6     gear\n6.carb_mean   3.4285714  1.8126539      6     carb\n8.mpg_mean   15.1000000  2.5600481      8      mpg\n8.cyl_mean    8.0000000  0.0000000      8      cyl\n8.disp_mean 353.1000000 67.7713236      8     disp\n8.hp_mean   209.2142857 50.9768855      8       hp\n8.drat_mean   3.2292857  0.3723618      8     drat\n8.wt_mean     3.9992143  0.7594047      8       wt\n8.qsec_mean  16.7721429  1.1960138      8     qsec\n8.vs_mean     0.0000000  0.0000000      8       vs\n8.am_mean     0.1428571  0.3631365      8       am\n8.gear_mean   3.2857143  0.7262730      8     gear\n8.carb_mean   3.5000000  1.5566236      8     carb\n\n#B) \n#to execute the dplyr we need to modify transpose_on_suffix\n# we need to account for column_summary to yield results for each level of our grouping variable. \n# This exercise is not for the feint of heart. For now lets go with plan A or the path of least resistance. Bonus: try to use an AI code helper to see how it would solve this task using dplyr and tidyr\n\n# data %>%\n#   mutate(groups=as.factor(cyl)) %>%\n#   group_by(groups) %>%\n#   column_summary() %>%\n#   transpose_on_suffix(.) # our original function needs to keep track of grouping variable levels. An easy solution is not obvious."
  },
  {
    "objectID": "data_wrangling.html#error-handling",
    "href": "data_wrangling.html#error-handling",
    "title": "3  Data Wrangling",
    "section": "3.3 Error handling",
    "text": "3.3 Error handling\nSometimes we may observe unexpected errors when executing functions over parts of the data (e.g. sample size is too low). We could handle this by checking and removing possible errors before hand or (simpler) handling errors in the calculation.\nLets take a moment to learn error handling.\n\n#the general form for for error handling using base R \n# tryCatch({\n#     expression # function call\n# }, warning = function(w){\n#     code that handles the warnings\n# }, error = function(e){\n#     code that handles the errors\n# }, finally = function(f){\n#     clean-up code\n# })\n\nf<-function(a){\n  a + 1\n}\n\ndata<-c(1:10)\nf(data)\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n# data<-c('1','a') # uncomment this to see an error message\n# f(data)\n\ntryCatch(f(data),error=function(e){print(as.character(e))}) # in this toy example the error is ignored and instead we print the error message as a string\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nNote, an alternative is to use the purrr:safely function which returns a more standard list consisting of results and error."
  },
  {
    "objectID": "data_wrangling.html#debugging",
    "href": "data_wrangling.html#debugging",
    "title": "3  Data Wrangling",
    "section": "3.4 Debugging",
    "text": "3.4 Debugging\nDebugging is the act of investigating how code functions or what causes errors. The browser and debug functions can be used to interactively run code and view its state.\n\n#browser can be used as a break point to pause code execution and overview its state\nf<-function(x){\n  x <- x + rnorm(1)\n  browser() # use c = continue, n = next line and Q = quit debugger\n  x\n  \n}\n\nf(2)\n\nCalled from: f(2)\ndebug at <text>#6: x\n\n\n[1] 2.216345\n\n#debug will sets a breakpoint any time a given function is run\n\nf<-function(x){\n  x <- x + rnorm(1)\n\n  x\n  \n}\n\ndebug(f)\n\nf(2)\n\ndebugging in: f(2)\ndebug at <text>#14: {\n    x <- x + rnorm(1)\n    x\n}\ndebug at <text>#15: x <- x + rnorm(1)\ndebug at <text>#17: x\nexiting from: f(2)\n\n\n[1] 1.492619"
  },
  {
    "objectID": "data_wrangling.html#reproducing-randomness",
    "href": "data_wrangling.html#reproducing-randomness",
    "title": "3  Data Wrangling",
    "section": "3.5 Reproducing randomness",
    "text": "3.5 Reproducing randomness\nMany R functions have random or stochastic components. The set.seed function can be used to reproduce function results with stochastic components.\n\nf<-function(){\n c(rnorm(1),rnorm(1)) \n}\n\nf()\n\n[1] -0.06006048  0.23333431\n\nf()\n\n[1] 1.1638466 0.5745557\n\nf<-function(seed=1){\n set.seed(seed)\n c(rnorm(1),rnorm(1)) \n}\n\nf()\n\n[1] -0.6264538  0.1836433\n\nf()\n\n[1] -0.6264538  0.1836433\n\nf(2) # different random seed\n\n[1] -0.8969145  0.1848492\n\nf(2) \n\n[1] -0.8969145  0.1848492\n\n#can also be used to set the global seed\nset.seed(1)\nc(rnorm(1),rnorm(1)) \n\n[1] -0.6264538  0.1836433\n\nset.seed(2)\nc(rnorm(1),rnorm(1)) \n\n[1] -0.8969145  0.1848492\n\n\nData wrangling is an inherent task for every data science workflow. We will build upon the data wrangling skills you have learned so far in the next sections."
  },
  {
    "objectID": "data_wrangling.html#appendix",
    "href": "data_wrangling.html#appendix",
    "title": "3  Data Wrangling",
    "section": "3.6 Appendix",
    "text": "3.6 Appendix\n\nR for Data Science"
  },
  {
    "objectID": "ggplot2_intro.html",
    "href": "ggplot2_intro.html",
    "title": "4  Plotting for data analysis",
    "section": "",
    "text": "While R is often described as a ~niche programming language (e.g. for statistics and bioinformatics), it shines for data visualization compared to other data analysis appropriate mainstream alternatives like python and julia. A major reason for this distinction is the ggplot2 library and its support for the grammar of graphics, an expressive, composable and extensible way to build data visualizations."
  },
  {
    "objectID": "ggplot2_intro.html#ggplot2",
    "href": "ggplot2_intro.html#ggplot2",
    "title": "4  Plotting for data analysis",
    "section": "4.1 ggplot2",
    "text": "4.1 ggplot2\nThe ggplot2 library supports rich combinations of graphical layers geoms, statistically derived layers, annotations, scales and aesthetic controls (just to name a few of its features).\nNext lets spend some time learning how to prepare and plot our data using ggplot2.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#load a demo data set\ndata(mtcars)\n\nLets visualize the relationship between data features and miles per gallon mpg. First, lets pick and arbitrary variable and explore how it relates to mpg.\n\nmtcars %>%\n  ggplot(.,aes(x=cyl,y=mpg))\n\n\n\n# # # we could also assign our plot to some variable and add to it later\n# p<-mtcars %>%\n#   ggplot(.,aes(x=cyl,y=mpg))\n# \n# p # render plot\n# p +\n#   ggtitle('Descriptive title goes here')\n\nThe first layer loads the data and global aesthetics (aes). The aes are an expressive way to define which data columns are plotted as x, y coordinates, colors, size, groups, etc."
  },
  {
    "objectID": "ggplot2_intro.html#scatter-plot",
    "href": "ggplot2_intro.html#scatter-plot",
    "title": "4  Plotting for data analysis",
    "section": "4.2 Scatter plot",
    "text": "4.2 Scatter plot\nThis plot does not show anything yet because we have not defined any layers to show (e.g. points or lines). Lets create a scatter plot showing the points with x and y positions defined by two columns in the data.\n\nmtcars %>% \n  ggplot(.,aes(x=cyl,y=mpg)) +\n  geom_point()"
  },
  {
    "objectID": "ggplot2_intro.html#box-plot",
    "href": "ggplot2_intro.html#box-plot",
    "title": "4  Plotting for data analysis",
    "section": "4.3 Box plot",
    "text": "4.3 Box plot\nSince we are plotting a categorical variable against a numeric variable using a box plot might be more informative. We can also change the default theme of the global plot.\n\nmtcars %>%\n  ggplot(.,aes(x=cyl,y=mpg,group=cyl)) + # the group is used to define if we want to create a seperate boxplot for each level in a column i.e. category\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\nThis is just the beginning of possibilities using ggplot2."
  },
  {
    "objectID": "ggplot2_intro.html#heatmap",
    "href": "ggplot2_intro.html#heatmap",
    "title": "4  Plotting for data analysis",
    "section": "4.4 Heatmap",
    "text": "4.4 Heatmap\nIt might be interesting to quantify and plot how all variables are correlated with mpg. Lets calculate non-parametric spearman correlations and show the results as a heatmap. Note, many custom libraries exist just for this task (e.g. heatmaply).\n\ncorr<-cor(mtcars,method = \"spearman\")\n\n#reshape the data into a 'melted' or long format\nlibrary(reshape2)\nmelted_corr <- melt(corr)\nhead(melted_corr)\n\n  Var1 Var2      value\n1  mpg  mpg  1.0000000\n2  cyl  mpg -0.9108013\n3 disp  mpg -0.9088824\n4   hp  mpg -0.8946646\n5 drat  mpg  0.6514555\n6   wt  mpg -0.8864220\n\n#plot\nggplot(data = melted_corr, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile()\n\n\n\n\nTo improve this visualization we may want to sort the plot dimensions (rows and columns) (e.g. using hierarchical clustering), only show a portion of the symmetric matrix (e.g. upper triangle) or change the color scales (e.g. change the color and only show correlations with p-value<= 0.05).\nLets show part of the square symmetric correlation matrix and improve the color scales.\n\n#plot the upper triangle\ncorr[upper.tri(corr)]<-0# set to zero the opposite quadrant you want to show, the plot will flip the symmetric values\nmelted_corr<-melt(corr)\n\nggplot(data = melted_corr, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", # specify color gradient\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n   name=\"Spearman\\nCorrelations\") + #add color to the element legend. Note this can also be done more generically using ggtitle().\n  theme_minimal()+ \n  # remove axis labels\n  ylab('') +\n  xlab('') +\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1)) + #rotate x-axis labels 45 degrees\n coord_fixed()\n\n\n\n\nThis is starting to look nice but we can better group related correlation patterns. We can do this using hierarchical clustering. The simplest way to do this is using the heatmaply R library.\n\nlibrary(heatmaply)\n\ncorr<-cor(mtcars,method = \"spearman\")\n### Let's Plot\nheatmaply_cor(x = corr,\n              xlab = \"Features\",\n              ylab = \"Features\",\n              k_col = 2,\n              k_row = 2)"
  },
  {
    "objectID": "ggplot2_intro.html#plotly",
    "href": "ggplot2_intro.html#plotly",
    "title": "4  Plotting for data analysis",
    "section": "4.5 Plotly",
    "text": "4.5 Plotly\nOne thing to notice is that the previous plot is interactive, which is achieved using the plotly library. We can convert any ggplot2 plot into an interactive plotly plot. Lets interactively explore another variable’s correlation with mpg.\n\nlibrary(plotly)\n\np<-mtcars %>% \n  ggplot(.,aes(x=disp,y=mpg)) +\n  stat_smooth(method = 'lm') + #show linear model fit\n  stat_smooth(method = 'loess') + #loess model fit (non-linear)\n  geom_point() + \n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\nNote the ggpmisc library offers convenience functions to plot the model coefficients and variance explained (R^2). Note the figure below is not based on the mtcars data set."
  },
  {
    "objectID": "ggplot2_intro.html#linear-model",
    "href": "ggplot2_intro.html#linear-model",
    "title": "4  Plotting for data analysis",
    "section": "4.6 Linear model",
    "text": "4.6 Linear model\nNext it might be interesting to visualize how cyl impacts the relationship between dispand mpg.\n\np<-mtcars %>% \n  mutate(cyl=as.factor(cyl)) %>% # convert to a factor to create sepearte models\n  ggplot(.,aes(x=disp,y=mpg,color=cyl)) +\n  stat_smooth(method = 'lm') + #show linear model fit\n  geom_point() + \n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\nThis suggests that mpg is best explained by the combination of disp and cyl. We can check this by making our own linear model.\n\nmod<-lm(mpg ~ disp ,data=mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 29.599855   1.229720  24.070  < 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\n\nmod2<-lm(mpg ~ disp + cyl +disp:cyl,data=mtcars)\nsummary(mod2)\n\n\nCall:\nlm(formula = mpg ~ disp + cyl + disp:cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0809 -1.6054 -0.2948  1.0546  5.7981 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 49.037212   5.004636   9.798 1.51e-10 ***\ndisp        -0.145526   0.040002  -3.638 0.001099 ** \ncyl         -3.405244   0.840189  -4.053 0.000365 ***\ndisp:cyl     0.015854   0.004948   3.204 0.003369 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.66 on 28 degrees of freedom\nMultiple R-squared:  0.8241,    Adjusted R-squared:  0.8052 \nF-statistic: 43.72 on 3 and 28 DF,  p-value: 1.078e-10\n\n#compare models\nanova(mod, mod2, test=\"Chisq\") # p-value denotes if the residual sum of squares are statistically significant (e.g. one model is better)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp\nModel 2: mpg ~ disp + cyl + disp:cyl\n  Res.Df    RSS Df Sum of Sq  Pr(>Chi)    \n1     30 317.16                           \n2     28 198.10  2    119.06 0.0002218 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can compare all possible linear models.\n\nsummary(mod <- lm(mpg ~ ., data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\nsmod <- step(mod,direction = 'both')\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- cyl   1    0.0799 147.57 68.915\n- vs    1    0.1601 147.66 68.932\n- carb  1    0.4067 147.90 68.986\n- gear  1    1.3531 148.85 69.190\n- drat  1    1.6270 149.12 69.249\n- disp  1    3.9167 151.41 69.736\n- hp    1    6.8399 154.33 70.348\n- qsec  1    8.8641 156.36 70.765\n<none>              147.49 70.898\n- am    1   10.5467 158.04 71.108\n- wt    1   27.0144 174.51 74.280\n\nStep:  AIC=68.92\nmpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- vs    1    0.2685 147.84 66.973\n- carb  1    0.5201 148.09 67.028\n- gear  1    1.8211 149.40 67.308\n- drat  1    1.9826 149.56 67.342\n- disp  1    3.9009 151.47 67.750\n- hp    1    7.3632 154.94 68.473\n<none>              147.57 68.915\n- qsec  1   10.0933 157.67 69.032\n- am    1   11.8359 159.41 69.384\n+ cyl   1    0.0799 147.49 70.898\n- wt    1   27.0280 174.60 72.297\n\nStep:  AIC=66.97\nmpg ~ disp + hp + drat + wt + qsec + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- carb  1    0.6855 148.53 65.121\n- gear  1    2.1437 149.99 65.434\n- drat  1    2.2139 150.06 65.449\n- disp  1    3.6467 151.49 65.753\n- hp    1    7.1060 154.95 66.475\n<none>              147.84 66.973\n- am    1   11.5694 159.41 67.384\n- qsec  1   15.6830 163.53 68.200\n+ vs    1    0.2685 147.57 68.915\n+ cyl   1    0.1883 147.66 68.932\n- wt    1   27.3799 175.22 70.410\n\nStep:  AIC=65.12\nmpg ~ disp + hp + drat + wt + qsec + am + gear\n\n       Df Sum of Sq    RSS    AIC\n- gear  1     1.565 150.09 63.457\n- drat  1     1.932 150.46 63.535\n<none>              148.53 65.121\n- disp  1    10.110 158.64 65.229\n- am    1    12.323 160.85 65.672\n- hp    1    14.826 163.35 66.166\n+ carb  1     0.685 147.84 66.973\n+ vs    1     0.434 148.09 67.028\n+ cyl   1     0.414 148.11 67.032\n- qsec  1    26.408 174.94 68.358\n- wt    1    69.127 217.66 75.350\n\nStep:  AIC=63.46\nmpg ~ disp + hp + drat + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- drat  1     3.345 153.44 62.162\n- disp  1     8.545 158.64 63.229\n<none>              150.09 63.457\n- hp    1    13.285 163.38 64.171\n+ gear  1     1.565 148.53 65.121\n+ cyl   1     1.003 149.09 65.242\n+ vs    1     0.645 149.45 65.319\n+ carb  1     0.107 149.99 65.434\n- am    1    20.036 170.13 65.466\n- qsec  1    25.574 175.67 66.491\n- wt    1    67.572 217.66 73.351\n\nStep:  AIC=62.16\nmpg ~ disp + hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- disp  1     6.629 160.07 61.515\n<none>              153.44 62.162\n- hp    1    12.572 166.01 62.682\n+ drat  1     3.345 150.09 63.457\n+ gear  1     2.977 150.46 63.535\n+ cyl   1     2.447 150.99 63.648\n+ vs    1     1.121 152.32 63.927\n+ carb  1     0.011 153.43 64.160\n- qsec  1    26.470 179.91 65.255\n- am    1    32.198 185.63 66.258\n- wt    1    69.043 222.48 72.051\n\nStep:  AIC=61.52\nmpg ~ hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- hp    1     9.219 169.29 61.307\n<none>              160.07 61.515\n+ disp  1     6.629 153.44 62.162\n+ carb  1     3.227 156.84 62.864\n+ drat  1     1.428 158.64 63.229\n- qsec  1    20.225 180.29 63.323\n+ cyl   1     0.249 159.82 63.465\n+ vs    1     0.249 159.82 63.466\n+ gear  1     0.171 159.90 63.481\n- am    1    25.993 186.06 64.331\n- wt    1    78.494 238.56 72.284\n\nStep:  AIC=61.31\nmpg ~ wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n<none>              169.29 61.307\n+ hp    1     9.219 160.07 61.515\n+ carb  1     8.036 161.25 61.751\n+ disp  1     3.276 166.01 62.682\n+ cyl   1     1.501 167.78 63.022\n+ drat  1     1.400 167.89 63.042\n+ gear  1     0.123 169.16 63.284\n+ vs    1     0.000 169.29 63.307\n- am    1    26.178 195.46 63.908\n- qsec  1   109.034 278.32 75.217\n- wt    1   183.347 352.63 82.790\n\nsummary(smod)\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4811 -1.5555 -0.7257  1.4110  4.6610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.6178     6.9596   1.382 0.177915    \nwt           -3.9165     0.7112  -5.507 6.95e-06 ***\nqsec          1.2259     0.2887   4.247 0.000216 ***\nam            2.9358     1.4109   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.459 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11"
  },
  {
    "objectID": "ggplot2_intro.html#model-optimization",
    "href": "ggplot2_intro.html#model-optimization",
    "title": "4  Plotting for data analysis",
    "section": "4.7 Model optimization",
    "text": "4.7 Model optimization\nThis a great data driven approach to hone in on the important variables to explain an objective of interest. Lets tune a scatter plot to best show the optimal model insights. We can use the model coefficient weights to prioritize what we show in the different layers. Lets first take a closer look at the variables.\n\nmtcars %>% \n  select(one_of(c('mpg','wt','qsec','am'))) %>%\n  head()# we can see am should be categorical\n\n                   mpg    wt  qsec am\nMazda RX4         21.0 2.620 16.46  1\nMazda RX4 Wag     21.0 2.875 17.02  1\nDatsun 710        22.8 2.320 18.61  1\nHornet 4 Drive    21.4 3.215 19.44  0\nHornet Sportabout 18.7 3.440 17.02  0\nValiant           18.1 3.460 20.22  0\n\n\nLooking at the variable types and levels in each category is helpful to decide which aes is best suited to visualize each dimension.\n\nmtcars %>% \n ggplot(.,aes(x=wt,y=mpg,size=qsec)) +\n  geom_point() +\n  facet_grid(.~am) +\n  stat_smooth(method='lm',show.legend = FALSE) +\n  theme_minimal()\n\n\n\n\n\n4.7.1 Scatter plot matrix\nWe can visualize all bivariate variable relationships using a scatter plot matrix.\n\nlibrary(GGally)\nlibrary(plotly)\ndata<-mtcars %>%\n  select(one_of(c('mpg','wt','qsec','am'))) %>%\n  mutate(am=as.factor(am))\n\np <- ggpairs(data, ggplot2::aes(colour=am) ) \n\nggplotly(p)\n\n\n\n\n\nVisualizing multivariate model term relationships can be useful to fine tune model interaction terms.\n\nmod3<-lm(mpg ~ wt * am * qsec, data=mtcars)\nsummary(mod3)\n\n\nCall:\nlm(formula = mpg ~ wt * am * qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3330 -1.4729 -0.4856  1.1495  4.0045 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -11.8073    57.7960  -0.204    0.840\nwt            3.8670    17.0915   0.226    0.823\nam           -5.5842    65.4915  -0.085    0.933\nqsec          2.2148     3.1734   0.698    0.492\nwt:am         4.0906    20.5487   0.199    0.844\nwt:qsec      -0.3805     0.9467  -0.402    0.691\nam:qsec       1.1768     3.6310   0.324    0.749\nwt:am:qsec   -0.5009     1.1648  -0.430    0.671\n\nResidual standard error: 2.123 on 24 degrees of freedom\nMultiple R-squared:  0.904, Adjusted R-squared:  0.8759 \nF-statistic: 32.27 on 7 and 24 DF,  p-value: 1.027e-10\n\n\nWhen we see a better fitting model based on Adjusted R-squared it is useful to consider the ~stability of the model coefficients. We can do this by comparing the coefficient weight relative to its standard deviation. We can do this based on the variance inflation factor.\n\nlibrary(car)\n(mod3_vif<-vif(mod3))\n\n        wt         am       qsec      wt:am    wt:qsec    am:qsec wt:am:qsec \n 1924.0248  7347.2244   221.2218  4632.5795  1873.1026  6918.8239  4169.4921 \n\n(smod_vif<-vif(smod))\n\n      wt     qsec       am \n2.482952 1.364339 2.541437 \n\n\nModel parameter weights with high vif suggest small changes in future data (e.g. another data set) will greatly influence the model predictions. While this is an exercise for another time, one could undertake it by sampling from the original data and adding some error to create synthetic test data then comparing predictions of mpg given this data between different models.\n\n\n4.7.2 Dumbbell plot\nLastly lets use the skills we learned so far to visualize the individual model vif values. One idea could be to create a dumbbell plot comparing two model’s values.\n\nlibrary(ggplot2)\nlibrary(ggalt)\ntheme_set(theme_minimal()) # set theme globaly\n\n#we need to extract the model coefficient and join them together\nd1<-smod_vif %>%\n  data.frame(term=names(.),vif1=.)\nd2<-mod3_vif %>%\n  data.frame(term=names(.),vif2=.)\n\n\n#we want to create two columns, one for each model's vif, for all terms\nlibrary(dplyr)\ndata<-data.frame(term=c(d1$term,d2$term) %>% unique()) %>%\n  left_join(d1,by='term') %>%\n  full_join(d2,by='term')\n\n#fix NA in model missing terms. Note this obfuscates which terms are presnt in the model.\ndata[is.na(data)]<-0\n\ngg <- ggplot(data, aes(x=vif1, xend=vif2, y=term, group=term)) + \n        geom_dumbbell(color=\"#a3c4dc\", \n                      size=0.75)\nplot(gg)\n\n\n\n\n\n\n4.7.3 Bar chart\nAlternatively we can make a bar chart to compare model vif.\n\n#we need to extract the model coefficient and join them together\nd1<-smod_vif %>%\n  data.frame(term=names(.),vif=.)\nd2<-mod3_vif %>%\n  data.frame(term=names(.),vif=.)\n\n\n#we want to create two columns, one for each model's vif, for all terms\ndata<-data.frame(term=c(d1$term,d2$term) %>% unique()) %>%\n  left_join(d1,by='term') %>%\n  left_join(d2,by='term')\n\n\ndata<-melt(data,id.vars=c('term'))\n\nggplot(data,aes(x=term, y=value,fill=variable,group=variable)) +\n  scale_y_log10() +\n  geom_bar(stat=\"identity\",position=position_dodge()) +\n  scale_fill_discrete(name = \"model\", labels = c('smod','mod3')) # custom legend titles\n\n\n\n\n\n\n4.7.4 Simulation\nNext lets test how predictive our models are based on synthetic data. Lets use a simple strategy to simulate data for each categorical variable based on the original data column mean + standard deviations * error.\n\n#Note you can use the short cut 'shift+ctrl+alt +R' (when in a function) to initialize Roxygen documentation \n\n#' Title\n#'\n#' @param data data frame of numeric values. Note, need to add special handling for characters or factors.\n#' @param error error in units of standard deviation\n#'\n#' @return data frame of simulate data based on sampling from the normal distribution\n#' @export\n#'\n#' @examples\nsimulate_data<-function(data,error=1,y=NULL){\n  \n  #loop over each column in and simulate n rows base original column mean and error\n  out<-lapply(data, function(x){\n    .mean<-mean(x,na.rm=TRUE)\n    .sd<-sd(x,na.rm=TRUE)\n    rnorm(nrow(data),.mean,.sd*error)\n    \n  }) %>%\n    do.call('cbind',.) \n  \n  #note we might not want to add error to out objective\n  if(!is.null(y)){\n    out[,y]<-data[,y]\n  }\n  \n  #add row names\n  row.names(out)<-rownames(data)\n\n  \n  out\n}\n\n#test - note this does not handle categorical data correctly\n# simulate_data(mtcars)\n\n#a simple fix is to simulate data for each categorical variable separately\nlibrary(tidyr)\ngroup<-c('am','vs','cyl','gear')\ndata<-mtcars %>%\n  unite(., 'group',group,remove=FALSE)\n\ntmp<-data %>% split(.,data$group)\nsim_data<-lapply(tmp,function(x){\n  simulate_data(data = x %>% select(-group))\n  \n}) %>%\n  do.call('rbind',.) %>%\n  na.omit() %>%\n  data.frame()\n\nhead(sim_data)\n\n                         mpg cyl     disp       hp     drat       wt     qsec\nHornet Sportabout   6.576782   8 168.6603 221.4418 2.747318 3.054902 16.83792\nDuster 360         14.611664   8 305.4927 194.9376 2.995105 3.801058 17.06951\nMerc 450SE         12.297373   8 457.4439 207.7557 3.049195 4.095190 18.28269\nMerc 450SL         17.955662   8 316.3608 144.6733 3.630278 5.561612 16.52407\nMerc 450SLC        18.124645   8 366.3572 132.0552 3.230379 3.911796 15.14880\nCadillac Fleetwood 15.222697   8 447.9372 227.0254 3.233176 3.210441 16.34552\n                   vs am gear     carb\nHornet Sportabout   0  0    3 3.920926\nDuster 360          0  0    3 3.449949\nMerc 450SE          0  0    3 2.645006\nMerc 450SL          0  0    3 3.357995\nMerc 450SLC         0  0    3 2.841518\nCadillac Fleetwood  0  0    3 2.691150\n\n#don't add error to y\nsim_data2<-lapply(tmp,function(x){\n  simulate_data(data = x %>% select(-group),y='mpg')\n  \n}) %>%\n  do.call('rbind',.) %>%\n  na.omit() %>%\n  data.frame()\n\n\n# #note see which groups we lost\n# dim(sim_data)\n# dim(mtcars)\n\n\n\n4.7.5 Prediction\nPredict mpg for the simulated data and compare model error.\n\npred1<-predict(mod3,sim_data)\npred2<-predict(smod,sim_data)\n\n#calculate error in original units of y - Root mean squared error (RMSE)\nlibrary(Metrics)\n\ny<-'mpg'\npred1_RMSE<-rmse(sim_data[,y,drop=TRUE],pred1 )\npred2_RMSE<-rmse(sim_data[,y,drop=TRUE],pred2 )\n\n# error in y\ndata.frame(model=c('mod3','smod'),RMSE=c(pred1_RMSE,pred2_RMSE))\n\n  model     RMSE\n1  mod3 4.088025\n2  smod 4.864466\n\n# no error in y\npred1<-predict(mod3,sim_data2)\npred2<-predict(smod,sim_data2)\npred1_RMSE<-rmse(sim_data2[,y,drop=TRUE],pred1 )\npred2_RMSE<-rmse(sim_data2[,y,drop=TRUE],pred2 )\n\ndata.frame(model=c('mod3','smod'),RMSE=c(pred1_RMSE,pred2_RMSE))\n\n  model     RMSE\n1  mod3 3.075280\n2  smod 2.968569\n\n\n\n\n4.7.6 Model diagnostics\nA useful analysis is to visualize the model residuals vs the actual values.\n\nlibrary(ggrepel) # improved text plotting\nresidual<-{sim_data[,y,drop=TRUE]- pred1} %>% # absolute value\n  data.frame(actual=sim_data[,y,drop=TRUE],residual=.,row_name=row.names(sim_data))  \n\nresid_plot<-ggplot(residual, aes(x=actual,y=residual)) +\n  geom_point() +\n  stat_smooth(color='gray') +\n  geom_text_repel(aes(label=row_name))\n\nresid_plot\n\n\n\n\nWe can also plot the predicted vs the actual values for each group of the simulated data.\n\npredicted<-predict(mod3,sim_data)\n\n\n#add group info\ngroup<-c('am','vs','cyl','gear')\ndata<-sim_data %>%\n  unite(., 'group',group,remove=FALSE) %>%\n  mutate(row_name=rownames(sim_data),predicted=predicted )\n\n\npred_plot<-ggplot(data, aes(y = mpg, x =predicted)) +\n  geom_point(aes(color = group), size = 3) +\n  stat_smooth(method = 'lm', color = 'gray') +\n  geom_text_repel(aes(label = row_name, color = group), show.legend = FALSE) +\n  scale_color_discrete(name = paste(group, collapse = '_')) +\n  geom_abline(slope = 1,intercept = 0,linetype='dashed')  #line if predicted and actual perfectly matched\n  \npred_plot\n\n\n\n\nFinally we could compare the density distributions and test for significant differences in model predictions.\n\n#we want to plot the difference from the true vs. predicted value, residual\nresid1<-abs(sim_data[,y,drop=TRUE]-pred1) %>% # absolute value\n  data.frame(model='mod3',residual=.)  \nresid2<-abs(sim_data2[,y,drop=TRUE]-pred2) %>% # absolute value\n  data.frame(model='smod',residual=.)  \n\ndata<-rbind(resid1,resid2)\n\ncols <- c(\"#F76D5E\", \"#72D8FF\") # custom colors as hex codes\n\n# Density areas without lines\ndist_plot<-ggplot(data, aes(x = residual, fill = model)) +\n  geom_density(alpha = 0.8, color = NA) +  # color is the border\n  scale_fill_manual(values = cols) # set custom colors\n\ndist_plot\n\n\n\n\nTest for difference in model errors.\n\nt.test(resid1$residual,resid2$residual) # note we may want to test the shifted log  (i.e. log(x+10)) of the residuals to make them normal or use a non-parametric test\n\n\n    Welch Two Sample t-test\n\ndata:  resid1$residual and resid2$residual\nt = 0.636, df = 52.308, p-value = 0.5276\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8304877  1.6013586\nsample estimates:\nmean of x mean of y \n 2.563967  2.178531 \n\n\nBased on this analysis we can conclude the smaller model (i.e. less terms) is not significantly different from the model with more terms. In practice we want to proceed with interpreting the simplest model with the lowest error."
  },
  {
    "objectID": "ggplot2_intro.html#combining-multiple-plots",
    "href": "ggplot2_intro.html#combining-multiple-plots",
    "title": "4  Plotting for data analysis",
    "section": "4.8 Combining multiple plots",
    "text": "4.8 Combining multiple plots\nThe pathchwork R library makes it easy to combine multiple plots using a variety of custom layouts. Combining plots is as easy as creating individual plots and then defining how they should be combined to create a single visualization based on their layout and position (e.g. in rows and/or columns).\n\nlibrary(patchwork)\n\n\npatchwork <- (dist_plot + resid_plot) / pred_plot\npatchwork + plot_annotation(\n  title = 'Model diagnostic plot',\n  subtitle = 'Comparison of model residual distributions, residuelas and predicted values',\n  caption = 'Example of what is possible'\n)"
  },
  {
    "objectID": "ggplot2_intro.html#appendix",
    "href": "ggplot2_intro.html#appendix",
    "title": "4  Plotting for data analysis",
    "section": "4.9 Appendix",
    "text": "4.9 Appendix\n\nggplot2 cheatsheet\nggplot2 tutorial"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory data analysis (EDA) is helpful for summarizing variables, accessing data quality, exploring multivariate trends and refining data analysis strategies."
  },
  {
    "objectID": "eda.html#data-summary",
    "href": "eda.html#data-summary",
    "title": "5  Exploratory Data Analysis",
    "section": "5.1 Data Summary",
    "text": "5.1 Data Summary\nWhen faced with a new data set, a great first step is to identify the types of variables and summarize them. Lets use this as an opportunity to practice the skills we have learned so far. Later we will use some R libraries to take our data summary skills to a new level.\n\n5.1.1 Overview and summary\n\nlibrary(dplyr)\n\ndata<-mtcars\ndata(data)\n\n#data structure\nstr(data)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n#variable types\nlapply(data,class)  %>%\n  unlist() %>%\n  setNames(.,names(data))\n\n      mpg       cyl      disp        hp      drat        wt      qsec        vs \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       am      gear      carb \n\"numeric\" \"numeric\" \"numeric\" \n\n#basic summary\nsummary(data)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nNext lets use the skimr library for a quick data overview.\n\n\n5.1.2 skimr\n\nlibrary(skimr)\n\nskim(data) \n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nDo you recognize the output format? This is the same as we previously created in the data wrangling section. Additional information includes overview of missing values, quantiles and variable histograms. This method is specific for different types.\n\nlibrary(dplyr)\nmtcars %>%\n  mutate(cyl=as.factor(cyl)) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncyl\n0\n1\nFALSE\n3\n8: 14, 4: 11, 6: 7\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nNotice how we get a separate summary for each variable type."
  },
  {
    "objectID": "eda.html#summarytools-summary",
    "href": "eda.html#summarytools-summary",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 summarytools summary",
    "text": "5.2 summarytools summary\n\nlibrary(summarytools)\n\n.summary <- dfSummary(data)\n# view(.summary)  #view html report"
  },
  {
    "objectID": "eda.html#data-quality",
    "href": "eda.html#data-quality",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Data Quality",
    "text": "5.3 Data Quality\nData quality assessment is an important first step for any analysis. Ideally the experimental design includes replicated quality control samples which can be used for this purpose. For this demo we will assess variability for a grouping variable.\n\n.summary<-mtcars %>%\n  mutate(cyl=as.factor(cyl)) %>%\n  group_by(cyl) %>%\n  skim()\n\n.summary\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\ncyl\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncyl\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n4\n0\n1\n26.66\n4.51\n21.40\n22.80\n26.00\n30.40\n33.90\n▇▃▂▃▃\n\n\nmpg\n6\n0\n1\n19.74\n1.45\n17.80\n18.65\n19.70\n21.00\n21.40\n▅▂▂▁▇\n\n\nmpg\n8\n0\n1\n15.10\n2.56\n10.40\n14.40\n15.20\n16.25\n19.20\n▂▁▇▃▂\n\n\ndisp\n4\n0\n1\n105.14\n26.87\n71.10\n78.85\n108.00\n120.65\n146.70\n▇▂▂▆▃\n\n\ndisp\n6\n0\n1\n183.31\n41.56\n145.00\n160.00\n167.60\n196.30\n258.00\n▇▁▁▂▂\n\n\ndisp\n8\n0\n1\n353.10\n67.77\n275.80\n301.75\n350.50\n390.00\n472.00\n▇▅▃▂▅\n\n\nhp\n4\n0\n1\n82.64\n20.93\n52.00\n65.50\n91.00\n96.00\n113.00\n▃▆▁▇▃\n\n\nhp\n6\n0\n1\n122.29\n24.26\n105.00\n110.00\n110.00\n123.00\n175.00\n▇▃▁▁▂\n\n\nhp\n8\n0\n1\n209.21\n50.98\n150.00\n176.25\n192.50\n241.25\n335.00\n▇▂▃▁▁\n\n\ndrat\n4\n0\n1\n4.07\n0.37\n3.69\n3.81\n4.08\n4.16\n4.93\n▇▅▃▁▂\n\n\ndrat\n6\n0\n1\n3.59\n0.48\n2.76\n3.35\n3.90\n3.91\n3.92\n▂▂▁▂▇\n\n\ndrat\n8\n0\n1\n3.23\n0.37\n2.76\n3.07\n3.12\n3.22\n4.22\n▃▇▁▁▁\n\n\nwt\n4\n0\n1\n2.29\n0.57\n1.51\n1.89\n2.20\n2.62\n3.19\n▇▅▇▂▅\n\n\nwt\n6\n0\n1\n3.12\n0.36\n2.62\n2.82\n3.21\n3.44\n3.46\n▅▂▁▂▇\n\n\nwt\n8\n0\n1\n4.00\n0.76\n3.17\n3.53\n3.76\n4.01\n5.42\n▇▇▁▁▃\n\n\nqsec\n4\n0\n1\n19.14\n1.68\n16.70\n18.56\n18.90\n19.95\n22.90\n▃▇▇▁▂\n\n\nqsec\n6\n0\n1\n17.98\n1.71\n15.50\n16.74\n18.30\n19.17\n20.22\n▃▇▃▃▇\n\n\nqsec\n8\n0\n1\n16.77\n1.20\n14.50\n16.10\n17.18\n17.56\n18.00\n▂▂▁▅▇\n\n\nvs\n4\n0\n1\n0.91\n0.30\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nvs\n6\n0\n1\n0.57\n0.53\n0.00\n0.00\n1.00\n1.00\n1.00\n▆▁▁▁▇\n\n\nvs\n8\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nam\n4\n0\n1\n0.73\n0.47\n0.00\n0.50\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nam\n6\n0\n1\n0.43\n0.53\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n8\n0\n1\n0.14\n0.36\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ngear\n4\n0\n1\n4.09\n0.54\n3.00\n4.00\n4.00\n4.00\n5.00\n▁▁▇▁▂\n\n\ngear\n6\n0\n1\n3.86\n0.69\n3.00\n3.50\n4.00\n4.00\n5.00\n▃▁▇▁▂\n\n\ngear\n8\n0\n1\n3.29\n0.73\n3.00\n3.00\n3.00\n3.00\n5.00\n▇▁▁▁▁\n\n\ncarb\n4\n0\n1\n1.55\n0.52\n1.00\n1.00\n2.00\n2.00\n2.00\n▇▁▁▁▇\n\n\ncarb\n6\n0\n1\n3.43\n1.81\n1.00\n2.50\n4.00\n4.00\n6.00\n▃▁▇▁▂\n\n\ncarb\n8\n0\n1\n3.50\n1.56\n2.00\n2.25\n3.50\n4.00\n8.00\n▇▇▁▁▁\n\n\n\n\n\nWe used group_by to summarize trends for each level of our grouping variable cyl. Notice how the data is formatted. This is referred to as a melted or long data format. Next, lets calculate the coefficient of variation (CV; std/mean) for each column in our data and level of cyl.\n\n#calculate CV\nCV <-.summary %>%\n select(contains(c('mean','sd'))) %>%\n  {(.[2]/.[1]) *100} %>%\n  setNames(.,'CV')\n\n.summary['CV']<- CV\n\nPlot the CV vs. the mean for each variable separately for each level of cyl.\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\ntheme_set(theme_minimal()) # set theme globaly\noptions(repr.plot.width = 2, repr.plot.height =3) # globaly set wi\n\nggplot(.summary, aes(x=numeric.mean,y=CV,color=cyl)) + \n  geom_point(size=2, alpha=.75) +\n  geom_text_repel(aes(label=skim_variable),show.legend = FALSE) +\n  facet_grid(.~cyl) +\n  scale_color_brewer(palette = 'Set1') +\n  xlab('mean') +\n  ylab('Coefficient of variation')\n\n\n\n\nThis plot is useful to identify variables with low precision which may need to be omitted for further analyses. In the case of mtcars we see the variables with high CV compared to their mean should all be categorical. For example we can check that am shows a large differences for levels of cyl.\n\nmtcars %>%\n  select(one_of(c('cyl','am'))) %>%\n  table()\n\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Multivariate Analysis",
    "text": "5.4 Multivariate Analysis\nNext lets visualize sample (row) trends given all variables (columns) using principal components analysis (PCA). First, lets calculate the principal components and visualize their variance explained.\n\n#calculate and show eigenvalue summary\npca_obj <- prcomp(data,scale= TRUE)\n\n\n5.4.1 Visualize optimal principal components (PCs) to retain.\n\n#notice the summary method does not return the results as printed.\n#we could modify the method todo so or replicate results\neigenvals<-pca_obj$sdev\neigenvals_cumsum<-cumsum(eigenvals)\nvar_explained<-sum(eigenvals)\nprop_var_exp<-eigenvals/var_explained\nprop_cumsum<-eigenvals_cumsum/var_explained\n\npca_eigen<-data.frame(PC=1:length(eigenvals),var_explained=prop_var_exp,eigen_cumsum=prop_cumsum)\n\nPlot eigenvalues to select total number of PCs\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\ndf<-melt(pca_eigen,id.vars = 'PC')\n\nggplot(df, aes(\n  x = as.factor(PC),\n  y = value,\n  fill = variable,\n  group = variable\n)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_fill_brewer(\n    palette = 'Set1',\n    labels = c('variance explained', 'cummulative variance\\nexplained')\n  ) +\n  theme_minimal() +\n  geom_hline(yintercept = .8, linetype = \"dashed\") +\n  xlab('Principal Components')"
  },
  {
    "objectID": "eda.html#create-a-scatter-plot-matrix-to-visualize-pc-scores",
    "href": "eda.html#create-a-scatter-plot-matrix-to-visualize-pc-scores",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Create a scatter plot matrix to visualize PC scores",
    "text": "5.5 Create a scatter plot matrix to visualize PC scores\nVisualize each pairwise PC comparison and color sample scores by cyl.\n\nlibrary(GGally)\nlibrary(plotly)\n\nlimit<-5\ngroup<-'cyl'\n\ndf <- pca_obj$x[,1:limit] %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) %>%\n  left_join(mtcars %>% select(one_of(group)) %>% mutate(id = rownames(.))) %>%\n  select(-id) %>%\n  mutate(group=as.factor(!!sym(group)))\n\n  \np <- ggpairs(df, ggplot2::aes(colour=group ))\n\nggplotly(p)\n\n\n\n\n\n\n5.5.1 Visualize specific PC sample scores\nPlot the principal plane (PC1 vs PC2) and identify scores for different numbers of cyl with ellipses.\n\nlibrary(ggrepel)\nscores_df <- pca_obj$x %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) %>%\n  left_join(mtcars %>% mutate(id = rownames(.), cyl = as.factor(cyl)))\n\nggplot(scores_df, aes(x = PC1, y = PC2, color = cyl)) +\n  geom_point(size=3) +\n  geom_text_repel(aes(label=id), show.legend = FALSE, max.overlaps = nrow(df),force=100) +\n  stat_ellipse(aes(fill=cyl), geom = \"polygon\",alpha=.25, show.legend = FALSE) +\n  ggtitle('Scores')\n\n\n\n\nThis visualization is helpful for identify similarity within and between different numbers of cyl. For example, we can see that the biggest differences (in variables) are between cyl=4 and cyl=8. If we expect sample scores to be bivariate normal in the scores space, samples outside the ellipses can be helpful for identify moderate outliers among groups of cyl.\n\n\n5.5.2 Visualize variable loadings\n\ndf <- pca_obj$rotation %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) \n\nggplot(df, aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_text_repel(aes(label=id), show.legend = FALSE) +\n  ggtitle('Loadings')\n\n\n\n\n\n\n5.5.3 Create a custom component for axis labels\nWe will show the percent explained for each PC.\n\nprcomp_axis_label <- function(obj,digits=1) {\n  \n  n<-obj %>% length()\n  .name<-\n    paste0(\n    rep('PC',n),\n    c(1:n)\n  )\n  \n  paste0(.name,'[',round(obj,digits),'%]') %>%\n    setNames(.name)\n\n}\n\nprcomp_axis_label(pca_eigen$var_explained*100,0)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n\"PC1[33%]\" \"PC2[21%]\" \"PC3[10%]\"  \"PC4[7%]\"  \"PC5[6%]\"  \"PC6[6%]\"  \"PC7[5%]\" \n       PC8        PC9       PC10       PC11 \n \"PC8[4%]\"  \"PC9[4%]\" \"PC10[3%]\" \"PC11[2%]\" \n\n\n\nmy_labels<-prcomp_axis_label(pca_eigen$var_explained*100,0)\nx <-'PC1'\ny<-'PC2'\n\ndf<-df %>%\n  select(one_of(c(x,y,'id')))\n\nggplot(df, aes_string(x = x, y = y)) +\n  geom_point() +\n  geom_text_repel(aes(label=id), show.legend = FALSE, max.overlaps = nrow(df),force=3) +\n  ggtitle('Variable loadings') +\n  ylab(my_labels[y]) +\n  xlab(my_labels[x])\n\n\n\n\nLoadings can be used to identify which variables are most different between groups of sample scores. For example, since groups of cyl scores are spread in the x-axis (PC1) we can look at the variables with largest loadings on PC1 (largest negative and positive PC1 values (position on the x-axis) to identify the largest differences in variables between groups of cyl. This can be useful to identify that cars with smaller number of cyl have higher mpg and lower disp.\nWe can investigate this observation by making a custom visualization.\n\np<-ggplot(mtcars, aes(x = disp, y = mpg,color=as.factor(cyl))) +\n  geom_point(size=3)\n\np\n\n\n\n\nNotice how we can almost perfectly separate groups of cyl based on these two dimensions?\n\nlibrary(tidyr)\n#calculate min and max for variables given groups of cyl\n.ranges<-mtcars %>% \n  group_by(cyl) %>% \n  summarise(min_mpg = min(mpg), max_mpg = max(mpg),\n            min_disp = min(disp), max_disp = max(disp)) %>%\n  gather(variable, value, -cyl) %>% \n  separate(variable, into = c(\"variable\", \"stat\"), sep = \"_\") %>% \n  spread(stat, value)\n\n\n#add rectangles to the plot\n#note: this is sub optimal as we need to know what is x or y axis in the plot\ntmp<- .ranges %>%\n  split(., .$cyl)\n\n#need to know colors to set rectangle colors\np<-p + \n  scale_colour_brewer(\n  palette = 'Set1',\n  aesthetics = \"colour\"\n)\n#get color codes\nlibrary(RColorBrewer)\n.colors<-brewer.pal(length(levels(mtcars$cyl)), 'Set1')\n\nfor(x in 1:length(tmp)){\n  \n  i<-tmp[[x]]\n  xmin<- i %>%\n    filter(variable == 'min') %>%\n    select(disp) %>% \n    .[1,,drop=TRUE]\n  xmax<- i %>%\n    filter(variable == 'max') %>%\n    select(disp) %>% \n    .[1,,drop=TRUE]\n  ymin<- i %>%\n    filter(variable == 'min') %>%\n    select(mpg) %>% \n    .[1,,drop=TRUE]\n  ymax<- i %>%\n    filter(variable == 'max') %>%\n    select(mpg) %>% \n    .[1,,drop=TRUE]\n    \np<-p +\n  annotate(\"rect\", xmin = xmin , xmax = xmax, ymin = ymin, ymax = ymax,\n           alpha = .5,fill = .colors[x])\n  \n}\n\n\np + guides(color=guide_legend(title=\"number of cylinders\"))"
  },
  {
    "objectID": "eda.html#appendix",
    "href": "eda.html#appendix",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Appendix",
    "text": "5.6 Appendix\n\nsummarytools\nskimr"
  }
]