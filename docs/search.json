[
  {
    "objectID": "R_studio.html",
    "href": "R_studio.html",
    "title": "1  Rstudio",
    "section": "",
    "text": "DALL·E 2023-04-12 20.55.50 - hacker unicorn writing R code using the Rstudio IDE, cyberpunk\n\n\nRstudio is one the most comprehensive free interactive development environments (IDE) for R.\nThe following reference covers he major steps to install and configure your R studio.\nMajor steps include:\n\ninstall the latest version of R\ninstall Rstudio\nconfigure Rstudio defaults and appearance\n\nOnce you have Rstudio installed you can select Tools >> Global options and configure your preferences e.g. apperance."
  },
  {
    "objectID": "openapi.html",
    "href": "openapi.html",
    "title": "7  AI coding help",
    "section": "",
    "text": "There is a lot of hype around tools like chatGPT which is a class of large language models (LLM's) for usingnatural language inputs to answer questions, including how to write code."
  },
  {
    "objectID": "openapi.html#install-chatgpt-rstudio-add-in",
    "href": "openapi.html#install-chatgpt-rstudio-add-in",
    "title": "7  AI coding help",
    "section": "7.1 Install chatGPT Rstudio add in",
    "text": "7.1 Install chatGPT Rstudio add in\n\n7.1.1 Prerequisites\n\nMake an OpenAI account.\nCreate an OpenAI API key to use with the package.\nSet the API key up in Rstudio\n\n\n# install.packages(c(\"gptstudio\",\"waiter\"))\nlibrary(gptstudio)"
  },
  {
    "objectID": "openapi.html#chatgpt-rstudio-add-in",
    "href": "openapi.html#chatgpt-rstudio-add-in",
    "title": "7  AI coding help",
    "section": "7.2 chatGPT Rstudio add in",
    "text": "7.2 chatGPT Rstudio add in\n\nTry to ask the openAI LLM how it would code up some of the examples you have seen so far. Try running the code, giving feedback about any errors to the chatbot and see if you can customize and/or improve the code you have seen so far."
  },
  {
    "objectID": "openapi.html#appendix",
    "href": "openapi.html#appendix",
    "title": "7  AI coding help",
    "section": "7.3 Appendix",
    "text": "7.3 Appendix\n\ngptstudio\nOpenAI"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R programming for Data Science - A Beginner’s Guide",
    "section": "",
    "text": "This course covers the major topics for beginners to get started with R programming for data analysis and visualization.\nTopics:\n\nHow to get up and running with R using the Rstudio\nUnderstand R data types and functions\nUse data frame manipulation to wrangle data\nVisualize and plot with ggplot2 and plotly\nExploratory Data Analysis\nPut it all together by creating a reproducible data analysis reports\nUse AI tools to improve your code\n\n\n\n\nR >= 4.2.1\nRStudio >= 2022.12.0 Build 353\n\nSee section on Rsudio to get more details."
  },
  {
    "objectID": "data_wrangling.html#data",
    "href": "data_wrangling.html#data",
    "title": "3  Data Wrangling",
    "section": "3.1 Data",
    "text": "3.1 Data\nThe best way to learn how to manipulate data in R is to find a simple data set and practice different transformations. The datastes R package has many examples to experiment with.\nWhile we will work with demo data for the rest of the examples, in practice you will likely want to import your own custom data sets.\nTake a look at some available data sets in the datastes R package.\n\nif(!require(datasets)) install.packages('datasets') #install package for the first time\nlibrary(datasets) # load package in session\n\n.data<-data()\n# str(.data) #take a look at the resulting object \nhead(.data$results[,c('Item','Title')]) #extract specific elements\n\n     Item                    \n[1,] \"billboard\"             \n[2,] \"cms_patient_care\"      \n[3,] \"cms_patient_experience\"\n[4,] \"construction\"          \n[5,] \"fish_encounters\"       \n[6,] \"household\"             \n     Title                                                   \n[1,] \"Song rankings for Billboard top 100 in the year 2000\"  \n[2,] \"Data from the Centers for Medicare & Medicaid Services\"\n[3,] \"Data from the Centers for Medicare & Medicaid Services\"\n[4,] \"Completed construction in the US in 2018\"              \n[5,] \"Fish encounters\"                                       \n[6,] \"Household data\"                                        \n\n\nLets find some data about cars.\n\n#lets look for a key word (i.e. substring) in Title of the datasets\nkeyword<-'car'\n#try ?grepl to see how else it can be used\ncars_id<-grepl(keyword,as.data.frame(.data$result)$Title,ignore.case = T)\n.data$results[cars_id,]\n\n     Package    LibPath                                         \n[1,] \"tidyr\"    \"C:/Users/think/AppData/Local/R/win-library/4.2\"\n[2,] \"tidyr\"    \"C:/Users/think/AppData/Local/R/win-library/4.2\"\n[3,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n[4,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n[5,] \"datasets\" \"C:/Program Files/R/R-4.2.1/library\"            \n     Item                    \n[1,] \"cms_patient_care\"      \n[2,] \"cms_patient_experience\"\n[3,] \"CO2\"                   \n[4,] \"cars\"                  \n[5,] \"mtcars\"                \n     Title                                                   \n[1,] \"Data from the Centers for Medicare & Medicaid Services\"\n[2,] \"Data from the Centers for Medicare & Medicaid Services\"\n[3,] \"Carbon Dioxide Uptake in Grass Plants\"                 \n[4,] \"Speed and Stopping Distances of Cars\"                  \n[5,] \"Motor Trend Car Road Tests\"                            \n\n\nSee here for more examples of string processing in R.\nLets load and review the mtcars data set.\n\ndata(\"mtcars\")\n# View(mtcars) #table view for small data - uncomment this line to see an interactive table\n#note we can also look at this data in the environment tab of Rstudio\n\nSummarize the data.\n\nsummary(mtcars) # see more advanced data summaries in the `Exploratory Data Analysis` section\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nNext lets introduce a more readable way to link R functions. We will use the pipe operator %>%.\n\n#lets format the meant miles per gallon to two digits\nround(mean(mtcars$mpg),2)\n\n[1] 20.09\n\n# we can rewrite this as a pipe where x %>% f(.) is equivalent to f(x). The '.' can be implicit or used to denote the 'x' on the left side of the equation.\nmtcars$mpg %>% \n  mean(.) %>%\n  round(.,2)\n\n[1] 20.09\n\n\nNote %>% can be imported in different ways and depends on the magrittr library. A more recent R update now allows to call the pipe operator from the base library as |>.\nWe can use the R package dplyr to create a custom summary. Lets calculate the mean and standard deviation of each column.\n\nmy_summary<-mtcars %>%\n  summarise_each(., funs(mean,stdev=sd))\n\nmy_summary\n\n  mpg_mean cyl_mean disp_mean  hp_mean drat_mean wt_mean qsec_mean vs_mean\n1 20.09062   6.1875  230.7219 146.6875  3.596563 3.21725  17.84875  0.4375\n  am_mean gear_mean carb_mean mpg_stdev cyl_stdev disp_stdev hp_stdev\n1 0.40625    3.6875    2.8125  6.026948  1.785922   123.9387 68.56287\n  drat_stdev  wt_stdev qsec_stdev  vs_stdev  am_stdev gear_stdev carb_stdev\n1  0.5346787 0.9784574   1.786943 0.5040161 0.4989909  0.7378041     1.6152\n\n\nA better format could be to output the results as columns for each row which correspond to the original columns in the data. We can check the dplyrcheat-sheet to see what other data wrangling operations this package enables.\n\nmeans <- my_summary %>%\n  select(ends_with('_mean')) %>%\n  t()\n  \nstdevs <- my_summary %>%\n  select(!ends_with('_mean')) %>%\n t()\n\n(my_summary2 <-\n    data.frame(\n      variable = colnames(mtcars),\n      mean = means,\n      stdev = stdevs\n    ))\n\n          variable       mean       stdev\nmpg_mean       mpg  20.090625   6.0269481\ncyl_mean       cyl   6.187500   1.7859216\ndisp_mean     disp 230.721875 123.9386938\nhp_mean         hp 146.687500  68.5628685\ndrat_mean     drat   3.596563   0.5346787\nwt_mean         wt   3.217250   0.9784574\nqsec_mean     qsec  17.848750   1.7869432\nvs_mean         vs   0.437500   0.5040161\nam_mean         am   0.406250   0.4989909\ngear_mean     gear   3.687500   0.7378041\ncarb_mean     carb   2.812500   1.6152000\n\n\nIn addition to dplyr the tidyr package also offers many useful data manipulation functions.\ndplyr\n\ntidyr\n\nLets round the results and then create a summary as mean +/- stdev . To do this we will create our first function. A function simply executes (calls) on a set of inputs (arguments).\n\n# lets start with the logic and then convert it to a function\n\n#inputs\ndigits<-1\nx<-my_summary2[,2,drop=FALSE] # data to test with-- second column\n\n#step 1 - round\nx %>%\n round(.,digits)\n\n           mean\nmpg_mean   20.1\ncyl_mean    6.2\ndisp_mean 230.7\nhp_mean   146.7\ndrat_mean   3.6\nwt_mean     3.2\nqsec_mean  17.8\nvs_mean     0.4\nam_mean     0.4\ngear_mean   3.7\ncarb_mean   2.8\n\n#step 2 - combine two columns\nmy_summary2 %>%\n  select(one_of(c('mean','stdev'))) %>%\n  unite(.,'mean_sd',sep= \" +/- \")\n\n                                  mean_sd\nmpg_mean    20.090625 +/- 6.0269480520891\ncyl_mean      6.1875 +/- 1.78592164694654\ndisp_mean 230.721875 +/- 123.938693831382\nhp_mean     146.6875 +/- 68.5628684893206\ndrat_mean 3.5965625 +/- 0.534678736070971\nwt_mean     3.21725 +/- 0.978457442989697\nqsec_mean   17.84875 +/- 1.78694323609684\nvs_mean      0.4375 +/- 0.504016128774185\nam_mean     0.40625 +/- 0.498990917235846\ngear_mean    3.6875 +/- 0.737804065256947\ncarb_mean     2.8125 +/- 1.61519997763185\n\n#create a function to do both at the same time on arbitrary inputs\n#note we are using Roxygen syntax (ctrl+shift+alt+R) to also document our funtion which is relevant when making R packages\n\n#' summary_function\n#'\n#' @param x data.frame\n#' @param digits int, number of digits to round to\n#' @param name str, colum name of results\n#' @param sep str, what to separate the combined columns with\n#'\n#' @return data.frame where each column is rounded to `digits` and combined into a string collapsed on `sep`.\n#' @export\n#' @details Round each column to `digits` and combined all columns into a string collapsed on `sep`.\n#' @examples\nsummary_function <-\n  function(x,\n           digits,\n           name = 'mean_sd',\n           sep = ' +/- ') {\n    x %>%\n      summarise(across(), round(.,digits)) %>%\n      unite(.,col= !!name, sep = sep) # ... use !! or {{}} for string arguments passed to dplyr verbs read more: https://dplyr.tidyverse.org/articles/programming.html\n  }\n\n#call function\n(tmp <-\n    my_summary2 %>%\n    select(one_of(c('mean', 'stdev'))) %>%\n    summary_function(., digits=2)\n  )\n\n             mean_sd\n1     20.09 +/- 6.03\n2      6.19 +/- 1.79\n3  230.72 +/- 123.94\n4   146.69 +/- 68.56\n5       3.6 +/- 0.53\n6      3.22 +/- 0.98\n7     17.85 +/- 1.79\n8       0.44 +/- 0.5\n9       0.41 +/- 0.5\n10     3.69 +/- 0.74\n11     2.81 +/- 1.62\n\n#add created object to our data\n(my_summary2<- my_summary2  %>%\n  cbind(.,tmp)\n  )\n\n          variable       mean       stdev           mean_sd\nmpg_mean       mpg  20.090625   6.0269481    20.09 +/- 6.03\ncyl_mean       cyl   6.187500   1.7859216     6.19 +/- 1.79\ndisp_mean     disp 230.721875 123.9386938 230.72 +/- 123.94\nhp_mean         hp 146.687500  68.5628685  146.69 +/- 68.56\ndrat_mean     drat   3.596563   0.5346787      3.6 +/- 0.53\nwt_mean         wt   3.217250   0.9784574     3.22 +/- 0.98\nqsec_mean     qsec  17.848750   1.7869432    17.85 +/- 1.79\nvs_mean         vs   0.437500   0.5040161      0.44 +/- 0.5\nam_mean         am   0.406250   0.4989909      0.41 +/- 0.5\ngear_mean     gear   3.687500   0.7378041     3.69 +/- 0.74\ncarb_mean     carb   2.812500   1.6152000     2.81 +/- 1.62\n\n\nNote, it can also be useful to call a functions by their string names using do.call('function_name',list(<arguments>))."
  },
  {
    "objectID": "data_wrangling.html#loops",
    "href": "data_wrangling.html#loops",
    "title": "3  Data Wrangling",
    "section": "3.2 Loops",
    "text": "3.2 Loops\nWhen we executed a function over each column this is executed by looping the calculation n number of times where n is equal to the number of columns. While modern libraries like dplyr, tidyr and purrr do this internally, it is useful to understand how to create your own loops. The simplest way to loop is using the for function. Note R is a vectorized language and looping is often discouraged because its much slower; this approach is still very useful for prototyping complex and simpler to read, understand and debug code.\nLets use apply to mimic summarise.\n\nmeans<-apply(mtcars,2,mean) # the margin 1 == across rows or 2 == columns\nstdevs<-apply(mtcars,2,'sd') # functions or their names are supported\n\ndata.frame(variable=colnames(mtcars),mean=means,stdev=stdevs)\n\n     variable       mean       stdev\nmpg       mpg  20.090625   6.0269481\ncyl       cyl   6.187500   1.7859216\ndisp     disp 230.721875 123.9386938\nhp         hp 146.687500  68.5628685\ndrat     drat   3.596563   0.5346787\nwt         wt   3.217250   0.9784574\nqsec     qsec  17.848750   1.7869432\nvs         vs   0.437500   0.5040161\nam         am   0.406250   0.4989909\ngear     gear   3.687500   0.7378041\ncarb     carb   2.812500   1.6152000\n\n\nNext, lets repeat our column summary calculation using a for loop.\n\nresults<-list() #initialize an empty list to store results in. Note it is more efficient to make a list of the same length as the number of elements you want to store.\nfor (i in 1:ncol(mtcars)){\n  \n  results$mean[i]<-mtcars[,i] %>%\n    mean() # store results in position [i] in the list results element named 'mean'\n  results$sd[i]<-mtcars[,i] %>%\n    sd()\n}\n\ndata.frame(variable=colnames(mtcars),results)\n\n   variable       mean          sd\n1       mpg  20.090625   6.0269481\n2       cyl   6.187500   1.7859216\n3      disp 230.721875 123.9386938\n4        hp 146.687500  68.5628685\n5      drat   3.596563   0.5346787\n6        wt   3.217250   0.9784574\n7      qsec  17.848750   1.7869432\n8        vs   0.437500   0.5040161\n9        am   0.406250   0.4989909\n10     gear   3.687500   0.7378041\n11     carb   2.812500   1.6152000\n\n\nA lapply is more convenient and versatile version of a for loop.\n\nlapply(mtcars,function(x){\n  c(mean=mean(x),sd=sd(x))\n}) %>%\n  do.call('rbind',.) %>% #combine list elements rowwise; use 'cbind' to combine columnwise \n  data.frame(variable=colnames(mtcars),.) \n\n     variable       mean          sd\nmpg       mpg  20.090625   6.0269481\ncyl       cyl   6.187500   1.7859216\ndisp     disp 230.721875 123.9386938\nhp         hp 146.687500  68.5628685\ndrat     drat   3.596563   0.5346787\nwt         wt   3.217250   0.9784574\nqsec     qsec  17.848750   1.7869432\nvs         vs   0.437500   0.5040161\nam         am   0.406250   0.4989909\ngear     gear   3.687500   0.7378041\ncarb     carb   2.812500   1.6152000\n\n\nNext, we will build on our function to create summaries for groups of rows. Lets summarize miles per gallon mpg for cars with different number of cylinders cyl. First lets create the functions for the individual steps.\n\n#we need to regenerate our our original analysis\n#we can take this opportunity to functionalize all the steps\n#1) calculate mean and standard deviation of each column\n#2) pivot data\n#3) create a custom summary\n\n#1 - execute function(s) on each column\ncolumn_summary<-function(data,functions = c(mean=mean,stdev=sd)){\n  data %>%\n  summarise_each(., funs(!!!(functions))) # use !!! for functions or unquoted arguments\n  \n}\n\n#test #1 \n(x<-mtcars %>% \n  column_summary()\n)\n\n  mpg_mean cyl_mean disp_mean  hp_mean drat_mean wt_mean qsec_mean vs_mean\n1 20.09062   6.1875  230.7219 146.6875  3.596563 3.21725  17.84875  0.4375\n  am_mean gear_mean carb_mean mpg_stdev cyl_stdev disp_stdev hp_stdev\n1 0.40625    3.6875    2.8125  6.026948  1.785922   123.9387 68.56287\n  drat_stdev  wt_stdev qsec_stdev  vs_stdev  am_stdev gear_stdev carb_stdev\n1  0.5346787 0.9784574   1.786943 0.5040161 0.4989909  0.7378041     1.6152\n\n#2 format results\n#we can explicitly pass column names we want to separate or infer based on suffix\n#2 A. infer common suffix\nget_unique_suffix<-function(data,sep='_'){\n  tmp<-colnames(data) %>%  #get column names\n  strsplit(.,'_') %>% #split string on '_''\n  do.call('rbind',.) %>% # combine list elements row wise\n  .[,2] %>% #get second column, better to reference by name\n  unique() #get unique values\n}\n\n#test 2 A\nget_unique_suffix(x)\n\n#2 B transpose elements\ntranspose_on_suffix<-function(data,sep='_'){\n  \n  suffixes<- data %>%\n    get_unique_suffix(.,sep)\n  \n  #loop over suffixes and transpose\n  lapply(suffixes,function(x){\n    data %>%\n    select(ends_with(x)) %>%\n    t() # transpose operation, i.e. rotate rows to columns\n    \n  }) %>%\n  do.call('cbind',.) %>% # bind list elements columnwise\n    data.frame() %>% # make sure its a data.frame\n    setNames(.,suffixes) #set column names\n}\n\n#test 2 A\ntranspose_on_suffix(x)\n\n                mean       stdev\nmpg_mean   20.090625   6.0269481\ncyl_mean    6.187500   1.7859216\ndisp_mean 230.721875 123.9386938\nhp_mean   146.687500  68.5628685\ndrat_mean   3.596563   0.5346787\nwt_mean     3.217250   0.9784574\nqsec_mean  17.848750   1.7869432\nvs_mean     0.437500   0.5040161\nam_mean     0.406250   0.4989909\ngear_mean   3.687500   0.7378041\ncarb_mean   2.812500   1.6152000\n\n\nNext we will execute our workflow grouping by different number of cylinders cyl.\n\n#next lets use our first factor to group our data\nclass(mtcars$cyl) #we want to convert this class to a factor\n\n[1] \"numeric\"\n\n#factors are a categorical vectors which are used for grouping operations\nstr(as.factor(mtcars$cyl))\n\n Factor w/ 3 levels \"4\",\"6\",\"8\": 2 2 1 2 3 2 3 1 1 2 ...\n\n#we could A) create a custom loop or B) modify our origin to handle a grouping variable\n\n#A) \n#we will split the data into list elements for each group and execute our simple workflow \ndata<-mtcars # make this more generic\ntmp<-data %>%\n  mutate(groups=as.factor(cyl)) #note, we need to save to an intermediate object for split to play nice with dplyr\n\ntmp %>% \n  split(.,.$groups) %>%\n  lapply(.,function(x){\n    x %>% select(-groups) %>% #remove factor which will cause an issue -- native dplyr handles this for us\n      column_summary(.) %>%\n      transpose_on_suffix(.) %>%\n      mutate(groups=x$groups %>% unique(),variable=colnames(data)) #note, we lost the variable names during the calculation Some options to fix this are A) save and carry forward variables in the original calculation (best -- complicated) or B) set variables as our data column names (simple but hard for others to understand and verify as correct)\n  }) %>%\n  do.call('rbind',.)\n\n                   mean      stdev groups variable\n4.mpg_mean   26.6636364  4.5098277      4      mpg\n4.cyl_mean    4.0000000  0.0000000      4      cyl\n4.disp_mean 105.1363636 26.8715937      4     disp\n4.hp_mean    82.6363636 20.9345300      4       hp\n4.drat_mean   4.0709091  0.3654711      4     drat\n4.wt_mean     2.2857273  0.5695637      4       wt\n4.qsec_mean  19.1372727  1.6824452      4     qsec\n4.vs_mean     0.9090909  0.3015113      4       vs\n4.am_mean     0.7272727  0.4670994      4       am\n4.gear_mean   4.0909091  0.5393599      4     gear\n4.carb_mean   1.5454545  0.5222330      4     carb\n6.mpg_mean   19.7428571  1.4535670      6      mpg\n6.cyl_mean    6.0000000  0.0000000      6      cyl\n6.disp_mean 183.3142857 41.5624602      6     disp\n6.hp_mean   122.2857143 24.2604911      6       hp\n6.drat_mean   3.5857143  0.4760552      6     drat\n6.wt_mean     3.1171429  0.3563455      6       wt\n6.qsec_mean  17.9771429  1.7068657      6     qsec\n6.vs_mean     0.5714286  0.5345225      6       vs\n6.am_mean     0.4285714  0.5345225      6       am\n6.gear_mean   3.8571429  0.6900656      6     gear\n6.carb_mean   3.4285714  1.8126539      6     carb\n8.mpg_mean   15.1000000  2.5600481      8      mpg\n8.cyl_mean    8.0000000  0.0000000      8      cyl\n8.disp_mean 353.1000000 67.7713236      8     disp\n8.hp_mean   209.2142857 50.9768855      8       hp\n8.drat_mean   3.2292857  0.3723618      8     drat\n8.wt_mean     3.9992143  0.7594047      8       wt\n8.qsec_mean  16.7721429  1.1960138      8     qsec\n8.vs_mean     0.0000000  0.0000000      8       vs\n8.am_mean     0.1428571  0.3631365      8       am\n8.gear_mean   3.2857143  0.7262730      8     gear\n8.carb_mean   3.5000000  1.5566236      8     carb\n\n#B) \n#to execute the dplyr we need to modify transpose_on_suffix\n# we need to account for column_summary to yield results for each level of our grouping variable. \n# This exercise is not for the feint of heart. For now lets go with plan A or the path of least resistance. Bonus: try to use an AI code helper to see how it would solve this task using dplyr and tidyr\n\n# data %>%\n#   mutate(groups=as.factor(cyl)) %>%\n#   group_by(groups) %>%\n#   column_summary() %>%\n#   transpose_on_suffix(.) # our original function needs to keep track of grouping variable levels. An easy solution is not obvious."
  },
  {
    "objectID": "data_wrangling.html#error-handling",
    "href": "data_wrangling.html#error-handling",
    "title": "3  Data Wrangling",
    "section": "3.3 Error handling",
    "text": "3.3 Error handling\nSometimes we may observe unexpected errors when executing functions over parts of the data (e.g. sample size is too low). We could handle this by checking and removing possible errors before hand or (simpler) handling errors in the calculation.\nLets take a moment to learn error handling.\n\n#the general form for for error handling using base R \n# tryCatch({\n#     expression # function call\n# }, warning = function(w){\n#     code that handles the warnings\n# }, error = function(e){\n#     code that handles the errors\n# }, finally = function(f){\n#     clean-up code\n# })\n\nf<-function(a){\n  a + 1\n}\n\ndata<-c(1:10)\nf(data)\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n# data<-c('1','a') # uncomment this to see an error message\n# f(data)\n\ntryCatch(f(data),error=function(e){print(as.character(e))}) # in this toy example the error is ignored and instead we print the error message as a string\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nNote, an alternative is to use the purrr:safely function which returns a more standard list consisting of results and error."
  },
  {
    "objectID": "data_wrangling.html#debugging",
    "href": "data_wrangling.html#debugging",
    "title": "3  Data Wrangling",
    "section": "3.4 Debugging",
    "text": "3.4 Debugging\nDebugging is the act of investigating how code functions or what causes errors. The browser and debug functions can be used to interactively run code and view its state.\n\n#browser can be used as a break point to pause code execution and overview its state\nf<-function(x){\n  x <- x + rnorm(1)\n  browser() # use c = continue, n = next line and Q = quit debugger\n  x\n  \n}\n\nf(2)\n\nCalled from: f(2)\ndebug at <text>#6: x\n\n\n[1] 2.216345\n\n#debug will sets a breakpoint any time a given function is run\n\nf<-function(x){\n  x <- x + rnorm(1)\n\n  x\n  \n}\n\ndebug(f)\n\nf(2)\n\ndebugging in: f(2)\ndebug at <text>#14: {\n    x <- x + rnorm(1)\n    x\n}\ndebug at <text>#15: x <- x + rnorm(1)\ndebug at <text>#17: x\nexiting from: f(2)\n\n\n[1] 1.492619"
  },
  {
    "objectID": "data_wrangling.html#reproducing-randomness",
    "href": "data_wrangling.html#reproducing-randomness",
    "title": "3  Data Wrangling",
    "section": "3.5 Reproducing randomness",
    "text": "3.5 Reproducing randomness\nMany R functions have random or stochastic components. The set.seed function can be used to reproduce function results with stochastic components.\n\nf<-function(){\n c(rnorm(1),rnorm(1)) \n}\n\nf()\n\n[1] -0.06006048  0.23333431\n\nf()\n\n[1] 1.1638466 0.5745557\n\nf<-function(seed=1){\n set.seed(seed)\n c(rnorm(1),rnorm(1)) \n}\n\nf()\n\n[1] -0.6264538  0.1836433\n\nf()\n\n[1] -0.6264538  0.1836433\n\nf(2) # different random seed\n\n[1] -0.8969145  0.1848492\n\nf(2) \n\n[1] -0.8969145  0.1848492\n\n#can also be used to set the global seed\nset.seed(1)\nc(rnorm(1),rnorm(1)) \n\n[1] -0.6264538  0.1836433\n\nset.seed(2)\nc(rnorm(1),rnorm(1)) \n\n[1] -0.8969145  0.1848492\n\n\nData wrangling is an inherent task for every data science workflow. We will build upon the data wrangling skills you have learned so far in the next sections."
  },
  {
    "objectID": "data_wrangling.html#appendix",
    "href": "data_wrangling.html#appendix",
    "title": "3  Data Wrangling",
    "section": "3.6 Appendix",
    "text": "3.6 Appendix\n\nR for Data Science"
  },
  {
    "objectID": "ggplot2_intro.html",
    "href": "ggplot2_intro.html",
    "title": "4  Plotting for data analysis",
    "section": "",
    "text": "While R is often described as a ~niche programming language (e.g. for statistics and bioinformatics), it shines for data visualization compared to other data analysis appropriate mainstream alternatives like python and julia. A major reason for this distinction is the ggplot2 library and its support for the grammar of graphics, an expressive, composable and extensible way to build data visualizations."
  },
  {
    "objectID": "ggplot2_intro.html#ggplot2",
    "href": "ggplot2_intro.html#ggplot2",
    "title": "4  Plotting for data analysis",
    "section": "4.1 ggplot2",
    "text": "4.1 ggplot2\nThe ggplot2 library supports rich combinations of graphical layers geoms, statistically derived layers, annotations, scales and aesthetic controls (just to name a few of its features).\nNext lets spend some time learning how to prepare and plot our data using ggplot2. Check out more detailed examples.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#load a demo data set\ndata(mtcars)\n\nLets visualize the relationship between data features and miles per gallon mpg. First, lets pick and arbitrary variable and explore how it relates to mpg.\n\nmtcars %>%\n  ggplot(.,aes(x=cyl,y=mpg))\n\n\n\n# # # # we could also assign our plot to some variable and add to it later\n# p<-mtcars %>%\n#   ggplot(.,aes(x=cyl,y=mpg))\n# \n# p # render plot\n# p +\n#   ggtitle('Descriptive title goes here')\n\nThe first layer loads the data and global aesthetics (aes). The aes are an expressive way to define which data columns are plotted as x, y coordinates, colors, size, groups, etc."
  },
  {
    "objectID": "ggplot2_intro.html#scatter-plot",
    "href": "ggplot2_intro.html#scatter-plot",
    "title": "4  Plotting for data analysis",
    "section": "4.2 Scatter plot",
    "text": "4.2 Scatter plot\nThis plot does not show anything yet because we have not defined any layers to show (e.g. points or lines). Lets create a scatter plot showing the points with x and y positions defined by two columns in the data.\n\nmtcars %>% \n  ggplot(.,aes(x=cyl,y=mpg)) +\n  geom_point()"
  },
  {
    "objectID": "ggplot2_intro.html#box-plot",
    "href": "ggplot2_intro.html#box-plot",
    "title": "4  Plotting for data analysis",
    "section": "4.3 Box plot",
    "text": "4.3 Box plot\nSince we are plotting a categorical variable against a numeric variable using a box plot might be more informative. We can also change the default theme of the global plot.\n\nmtcars %>%\n  ggplot(.,aes(x=cyl,y=mpg,group=cyl)) + # the group is used to define if we want to create a seperate boxplot for each level in a column i.e. category\n  geom_boxplot() +\n  # facet_grid(am ~ vs) +\n  # geom_violin() +\n  theme_minimal()\n\n\n\n\nThis is just the beginning of possibilities using ggplot2."
  },
  {
    "objectID": "ggplot2_intro.html#heatmap",
    "href": "ggplot2_intro.html#heatmap",
    "title": "4  Plotting for data analysis",
    "section": "4.4 Heatmap",
    "text": "4.4 Heatmap\nIt might be interesting to quantify and plot how all variables are correlated with mpg. Lets calculate non-parametric spearman correlations and show the results as a heatmap. Note, many custom libraries exist just for this task (e.g. heatmaply).\n\ncorr<-cor(mtcars,method = \"spearman\")\n\n#reshape the data into a 'melted' or long format\nlibrary(reshape2)\nmelted_corr <- melt(corr)\nhead(melted_corr)\n\n  Var1 Var2      value\n1  mpg  mpg  1.0000000\n2  cyl  mpg -0.9108013\n3 disp  mpg -0.9088824\n4   hp  mpg -0.8946646\n5 drat  mpg  0.6514555\n6   wt  mpg -0.8864220\n\n#plot\nggplot(data = melted_corr, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile()\n\n\n\n\nWe could further improve visualization by only showing a portion of the symmetric matrix (e.g. upper triangle) and change the color scales to better highlight differences between positive and negative correlations (bonus: highlight correlations with p-value<= 0.05).\nLets show part of the square symmetric correlation matrix and improve the color scales.\n\n#plot the upper triangle\ncorr[upper.tri(corr)]<-0# set to zero the opposite quadrant you want to show, the plot will flip the symmetric values\nmelted_corr<-melt(corr)\n\nggplot(data = melted_corr, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", # specify color gradient\n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n   name=\"Spearman\\nCorrelations\") + #add color to the element legend. Note this can also be done more generically using ggtitle().\n  theme_minimal()+ \n  # remove axis labels\n  ylab('') +\n  xlab('') +\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1)) + #rotate x-axis labels 45 degrees\n coord_fixed()\n\n\n\n\nSo far we have included numeric continuous variables and categorical ones in our visualization. It could be better only show correlations among continuous variables and show how the categorical designations in a separate panel.\n\nlibrary(pheatmap)\nlibrary(purrr)\n\ndata<-mtcars \n\n# we want to select categorical variables  for row meta data\n# lets define this as any variables with n unique values <= some percent of the total data rows \nx<-mtcars$gear\nis_categorical<-function(x,cutoff=0.3){\n\n  (length(unique(x))/ length(x)) <= cutoff  \n  \n}\n\n\ncat_cols<-sapply(mtcars,is_categorical) %>%\n  names(.)[.]\n\n\n#purrr::map archive similar goal to lapply\nd2<-data %>%\n  select(one_of(cat_cols)) %>%\n  map(as.factor) %>%\n  bind_rows()  %>%# alternative to do.call/cbind pattern\n  data.frame()\n#add row names for indexing\nrownames(d2)<-rownames(data)\n\n#create fake column metadata\nd3<-data.frame(fake1 = rep(c('A','B'),length.out=ncol(data)))\nrownames(d3)<-colnames(data)\n\n\npheatmap(\n  data %>% select(-one_of(cat_cols)),\n  annotation_row = d2,\n  annotation_col = d3,\n  cluster_cols = FALSE, # control clustering\n  cluster_rows = FALSE # # control clustering\n)\n\n\n\n\nThis is starting to look nice but we can better group related correlation patterns (i.e. sort the plot dimensions (rows and columns) based on similarity of patterns across all variable relationships). We can do this using hierarchical clustering. The simplest way to do this is using the heatmaply R library.\n\nlibrary(heatmaply)\n\ncorr<-cor(mtcars,method = \"spearman\")\n### Let's Plot\nheatmaply_cor(x = corr,\n              xlab = \"Features\",\n              ylab = \"Features\",\n              k_col = 2,\n              k_row = 2)"
  },
  {
    "objectID": "ggplot2_intro.html#plotly",
    "href": "ggplot2_intro.html#plotly",
    "title": "4  Plotting for data analysis",
    "section": "4.5 Plotly",
    "text": "4.5 Plotly\nOne thing to notice is that the previous plot is interactive, which is achieved using the plotly library. We can convert any ggplot2 plot into an interactive plotly plot. Lets interactively explore another variable’s correlation with mpg.\n\nlibrary(plotly)\n\np<-mtcars %>% \n  ggplot(.,aes(x=disp,y=mpg)) +\n  stat_smooth(method = 'lm') + #show linear model fit\n  stat_smooth(method = 'loess') + #loess model fit (non-linear)\n  geom_point() + \n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\nNote the ggpmisc library offers convenience functions to plot the model coefficients and variance explained (R^2). Note the figure below is not based on the mtcars data set."
  },
  {
    "objectID": "ggplot2_intro.html#linear-model",
    "href": "ggplot2_intro.html#linear-model",
    "title": "4  Plotting for data analysis",
    "section": "4.6 Linear model",
    "text": "4.6 Linear model\nNext it might be interesting to visualize how cyl impacts the relationship between dispand mpg.\n\np<-mtcars %>% \n  mutate(cyl=as.factor(cyl)) %>% # convert to a factor to create sepearte models\n  ggplot(.,aes(x=disp,y=mpg,color=cyl)) +\n  stat_smooth(method = 'lm') + #show linear model fit\n  geom_point() + \n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\nThis suggests that mpg is best explained by the combination of disp and cyl. We can check this by making our own linear model.\n\nmod<-lm(mpg ~ disp ,data=mtcars)\nsummary(mod)\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 29.599855   1.229720  24.070  < 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10\n\nmod2<-lm(mpg ~ disp + cyl + disp:cyl,data=mtcars)\nsummary(mod2)\n\n\nCall:\nlm(formula = mpg ~ disp + cyl + disp:cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0809 -1.6054 -0.2948  1.0546  5.7981 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 49.037212   5.004636   9.798 1.51e-10 ***\ndisp        -0.145526   0.040002  -3.638 0.001099 ** \ncyl         -3.405244   0.840189  -4.053 0.000365 ***\ndisp:cyl     0.015854   0.004948   3.204 0.003369 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.66 on 28 degrees of freedom\nMultiple R-squared:  0.8241,    Adjusted R-squared:  0.8052 \nF-statistic: 43.72 on 3 and 28 DF,  p-value: 1.078e-10\n\n#compare models\nanova(mod, mod2, test=\"Chisq\") # p-value denotes if the residual sum of squares are statistically significant (e.g. one model is better)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp\nModel 2: mpg ~ disp + cyl + disp:cyl\n  Res.Df    RSS Df Sum of Sq  Pr(>Chi)    \n1     30 317.16                           \n2     28 198.10  2    119.06 0.0002218 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can compare all possible linear models.\n\nsummary(mod <- lm(mpg ~ ., data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\nsmod <- step(mod,direction = 'both')\n\nStart:  AIC=70.9\nmpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- cyl   1    0.0799 147.57 68.915\n- vs    1    0.1601 147.66 68.932\n- carb  1    0.4067 147.90 68.986\n- gear  1    1.3531 148.85 69.190\n- drat  1    1.6270 149.12 69.249\n- disp  1    3.9167 151.41 69.736\n- hp    1    6.8399 154.33 70.348\n- qsec  1    8.8641 156.36 70.765\n<none>              147.49 70.898\n- am    1   10.5467 158.04 71.108\n- wt    1   27.0144 174.51 74.280\n\nStep:  AIC=68.92\nmpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- vs    1    0.2685 147.84 66.973\n- carb  1    0.5201 148.09 67.028\n- gear  1    1.8211 149.40 67.308\n- drat  1    1.9826 149.56 67.342\n- disp  1    3.9009 151.47 67.750\n- hp    1    7.3632 154.94 68.473\n<none>              147.57 68.915\n- qsec  1   10.0933 157.67 69.032\n- am    1   11.8359 159.41 69.384\n+ cyl   1    0.0799 147.49 70.898\n- wt    1   27.0280 174.60 72.297\n\nStep:  AIC=66.97\nmpg ~ disp + hp + drat + wt + qsec + am + gear + carb\n\n       Df Sum of Sq    RSS    AIC\n- carb  1    0.6855 148.53 65.121\n- gear  1    2.1437 149.99 65.434\n- drat  1    2.2139 150.06 65.449\n- disp  1    3.6467 151.49 65.753\n- hp    1    7.1060 154.95 66.475\n<none>              147.84 66.973\n- am    1   11.5694 159.41 67.384\n- qsec  1   15.6830 163.53 68.200\n+ vs    1    0.2685 147.57 68.915\n+ cyl   1    0.1883 147.66 68.932\n- wt    1   27.3799 175.22 70.410\n\nStep:  AIC=65.12\nmpg ~ disp + hp + drat + wt + qsec + am + gear\n\n       Df Sum of Sq    RSS    AIC\n- gear  1     1.565 150.09 63.457\n- drat  1     1.932 150.46 63.535\n<none>              148.53 65.121\n- disp  1    10.110 158.64 65.229\n- am    1    12.323 160.85 65.672\n- hp    1    14.826 163.35 66.166\n+ carb  1     0.685 147.84 66.973\n+ vs    1     0.434 148.09 67.028\n+ cyl   1     0.414 148.11 67.032\n- qsec  1    26.408 174.94 68.358\n- wt    1    69.127 217.66 75.350\n\nStep:  AIC=63.46\nmpg ~ disp + hp + drat + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- drat  1     3.345 153.44 62.162\n- disp  1     8.545 158.64 63.229\n<none>              150.09 63.457\n- hp    1    13.285 163.38 64.171\n+ gear  1     1.565 148.53 65.121\n+ cyl   1     1.003 149.09 65.242\n+ vs    1     0.645 149.45 65.319\n+ carb  1     0.107 149.99 65.434\n- am    1    20.036 170.13 65.466\n- qsec  1    25.574 175.67 66.491\n- wt    1    67.572 217.66 73.351\n\nStep:  AIC=62.16\nmpg ~ disp + hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- disp  1     6.629 160.07 61.515\n<none>              153.44 62.162\n- hp    1    12.572 166.01 62.682\n+ drat  1     3.345 150.09 63.457\n+ gear  1     2.977 150.46 63.535\n+ cyl   1     2.447 150.99 63.648\n+ vs    1     1.121 152.32 63.927\n+ carb  1     0.011 153.43 64.160\n- qsec  1    26.470 179.91 65.255\n- am    1    32.198 185.63 66.258\n- wt    1    69.043 222.48 72.051\n\nStep:  AIC=61.52\nmpg ~ hp + wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n- hp    1     9.219 169.29 61.307\n<none>              160.07 61.515\n+ disp  1     6.629 153.44 62.162\n+ carb  1     3.227 156.84 62.864\n+ drat  1     1.428 158.64 63.229\n- qsec  1    20.225 180.29 63.323\n+ cyl   1     0.249 159.82 63.465\n+ vs    1     0.249 159.82 63.466\n+ gear  1     0.171 159.90 63.481\n- am    1    25.993 186.06 64.331\n- wt    1    78.494 238.56 72.284\n\nStep:  AIC=61.31\nmpg ~ wt + qsec + am\n\n       Df Sum of Sq    RSS    AIC\n<none>              169.29 61.307\n+ hp    1     9.219 160.07 61.515\n+ carb  1     8.036 161.25 61.751\n+ disp  1     3.276 166.01 62.682\n+ cyl   1     1.501 167.78 63.022\n+ drat  1     1.400 167.89 63.042\n+ gear  1     0.123 169.16 63.284\n+ vs    1     0.000 169.29 63.307\n- am    1    26.178 195.46 63.908\n- qsec  1   109.034 278.32 75.217\n- wt    1   183.347 352.63 82.790\n\nsummary(smod)\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4811 -1.5555 -0.7257  1.4110  4.6610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.6178     6.9596   1.382 0.177915    \nwt           -3.9165     0.7112  -5.507 6.95e-06 ***\nqsec          1.2259     0.2887   4.247 0.000216 ***\nam            2.9358     1.4109   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.459 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11"
  },
  {
    "objectID": "ggplot2_intro.html#model-optimization",
    "href": "ggplot2_intro.html#model-optimization",
    "title": "4  Plotting for data analysis",
    "section": "4.7 Model optimization",
    "text": "4.7 Model optimization\nThis a great data driven approach to hone in on the important variables to explain an objective of interest. Lets tune a scatter plot to best show the optimal model insights. We can use the model coefficient weights to prioritize what we show in the different layers. Lets first take a closer look at the variables.\n\nmtcars %>% \n  select(one_of(c('mpg','wt','qsec','am'))) %>%\n  head()# we can see am should be categorical\n\n                   mpg    wt  qsec am\nMazda RX4         21.0 2.620 16.46  1\nMazda RX4 Wag     21.0 2.875 17.02  1\nDatsun 710        22.8 2.320 18.61  1\nHornet 4 Drive    21.4 3.215 19.44  0\nHornet Sportabout 18.7 3.440 17.02  0\nValiant           18.1 3.460 20.22  0\n\n\nLooking at the variable types and levels in each category is helpful to decide which aes is best suited to visualize each dimension.\n\nmtcars %>% \n ggplot(.,aes(x=wt,y=mpg,size=qsec)) +\n  geom_point() +\n  facet_grid(.~am) +\n  stat_smooth(method='lm',show.legend = FALSE) +\n  theme_minimal()\n\n\n\n\n\n4.7.1 Scatter plot matrix\nWe can visualize all bivariate variable relationships using a scatter plot matrix.\n\nlibrary(GGally)\nlibrary(plotly)\ndata<-mtcars %>%\n  select(one_of(c('mpg','wt','qsec','am'))) %>%\n  mutate(am=as.factor(am))\n\np <- ggpairs(data, ggplot2::aes(colour=am) ) \n\nggplotly(p)\n\n\n\n\n\nVisualizing multivariate model term relationships can be useful to fine tune model interaction terms.\n\nmod3<-lm(mpg ~ wt * am * qsec, data=mtcars)\nsummary(mod3)\n\n\nCall:\nlm(formula = mpg ~ wt * am * qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3330 -1.4729 -0.4856  1.1495  4.0045 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -11.8073    57.7960  -0.204    0.840\nwt            3.8670    17.0915   0.226    0.823\nam           -5.5842    65.4915  -0.085    0.933\nqsec          2.2148     3.1734   0.698    0.492\nwt:am         4.0906    20.5487   0.199    0.844\nwt:qsec      -0.3805     0.9467  -0.402    0.691\nam:qsec       1.1768     3.6310   0.324    0.749\nwt:am:qsec   -0.5009     1.1648  -0.430    0.671\n\nResidual standard error: 2.123 on 24 degrees of freedom\nMultiple R-squared:  0.904, Adjusted R-squared:  0.8759 \nF-statistic: 32.27 on 7 and 24 DF,  p-value: 1.027e-10\n\n\nWhen we see a better fitting model based on Adjusted R-squared it is useful to consider the ~stability of the model coefficients. We can do this by comparing the coefficient weight relative to its standard deviation. We can do this based on the variance inflation factor.\n\nlibrary(car)\n(mod3_vif<-vif(mod3))\n\n        wt         am       qsec      wt:am    wt:qsec    am:qsec wt:am:qsec \n 1924.0248  7347.2244   221.2218  4632.5795  1873.1026  6918.8239  4169.4921 \n\n(smod_vif<-vif(smod))\n\n      wt     qsec       am \n2.482952 1.364339 2.541437 \n\n\nModel parameter weights with high vif suggest small changes in future data (e.g. another data set) will greatly influence the model predictions. While this is an exercise for another time, one could undertake it by sampling from the original data and adding some error to create synthetic test data then comparing predictions of mpg given this data between different models.\n\n\n4.7.2 Dumbbell plot\nLastly lets use the skills we learned so far to visualize the individual model vif values. One idea could be to create a dumbbell plot comparing two model’s values.\n\nlibrary(ggplot2)\nlibrary(ggalt)\ntheme_set(theme_minimal()) # set theme globaly\n\n#we need to extract the model coefficient and join them together\nd1<-smod_vif %>%\n  data.frame(term=names(.),vif1=.)\nd2<-mod3_vif %>%\n  data.frame(term=names(.),vif2=.)\n\n\n#we want to create two columns, one for each model's vif, for all terms\nlibrary(dplyr)\ndata<-data.frame(term=c(d1$term,d2$term) %>% unique()) %>%\n  left_join(d1,by='term') %>%\n  full_join(d2,by='term') # left_join also works\n\n#fix NA in model missing terms. Note this obfuscates which terms are presnt in the model.\ndata[is.na(data)]<-0\n\ngg <- ggplot(data, aes(x=vif1, xend=vif2, y=term, group=term)) + \n        geom_dumbbell(color=\"#a3c4dc\", \n                      size=0.75)\nplot(gg)\n\n\n\n\n\n\n4.7.3 Bar chart\nAlternatively we can make a bar chart to compare model vif.\n\n#we need to extract the model coefficient and join them together\nd1<-smod_vif %>%\n  data.frame(term=names(.),vif=.)\nd2<-mod3_vif %>%\n  data.frame(term=names(.),vif=.)\n\n\n#we want to create two columns, one for each model's vif, for all terms\ndata<-data.frame(term=c(d1$term,d2$term) %>% unique()) %>%\n  left_join(d1,by='term') %>%\n  left_join(d2,by='term')\n\n\ndata<-melt(data,id.vars=c('term'))\n\nggplot(data,aes(x=term, y=value,fill=variable,group=variable)) +\n  scale_y_log10() +\n  geom_bar(stat=\"identity\",position=position_dodge()) +\n  scale_fill_discrete(name = \"model\", labels = c('smod','mod3')) # custom legend titles\n\n\n\n\n\n\n4.7.4 Simulation\nNext lets test how predictive our models are based on synthetic data. Lets use a simple strategy to simulate data for each categorical variable based on the original data column mean + standard deviations * error.\n\n#Note you can use the short cut 'shift+ctrl+alt +R' (when in a function) to initialize Roxygen documentation \n\n#' Title\n#'\n#' @param data data frame of numeric values. Note, need to add special handling for characters or factors.\n#' @param error error in units of standard deviation\n#'\n#' @return data frame of simulate data based on sampling from the normal distribution\n#' @export\n#'\n#' @examples\nsimulate_data<-function(data,error=1,y=NULL){\n  \n  #loop over each column in and simulate n rows base original column mean and error\n  out<-lapply(data, function(x){\n    .mean<-mean(x,na.rm=TRUE)\n    .sd<-sd(x,na.rm=TRUE)\n    rnorm(nrow(data),.mean,.sd*error)\n    \n  }) %>%\n    do.call('cbind',.) \n  \n  #note we might not want to add error to out objective\n  if(!is.null(y)){\n    out[,y]<-data[,y]\n  }\n  \n  #add row names\n  row.names(out)<-rownames(data)\n\n  \n  out\n}\n\n#test - note this does not handle categorical data correctly\n# simulate_data(mtcars)\n\n#a simple fix is to simulate data for each categorical variable separately\nlibrary(tidyr)\ngroup<-c('am','vs','cyl','gear')\ndata<-mtcars %>%\n  unite(., 'group',group,remove=FALSE)\n\ntmp<-data %>% split(.,data$group)\nsim_data<-lapply(tmp,function(x){\n  simulate_data(data = x %>% select(-group))\n  \n}) %>%\n  do.call('rbind',.) %>%\n  na.omit() %>%\n  data.frame()\n\nhead(sim_data)\n\n                         mpg cyl     disp       hp     drat       wt     qsec\nHornet Sportabout  14.015426   8 348.9829 231.5790 3.071433 4.505286 16.42136\nDuster 360         18.654248   8 439.5638 215.4694 2.997880 4.326504 17.05341\nMerc 450SE          9.627556   8 373.8116 168.8879 3.612832 3.923598 17.68837\nMerc 450SL         14.842060   8 410.5207 187.8549 2.973160 3.168750 17.72117\nMerc 450SLC        13.201534   8 350.9929 199.0579 2.861490 3.959033 17.14846\nCadillac Fleetwood 15.884363   8 402.7452 218.6817 2.957490 4.070591 15.89467\n                   vs am gear     carb\nHornet Sportabout   0  0    3 2.544713\nDuster 360          0  0    3 3.636987\nMerc 450SE          0  0    3 3.283955\nMerc 450SL          0  0    3 1.704567\nMerc 450SLC         0  0    3 2.748998\nCadillac Fleetwood  0  0    3 1.879934\n\n#don't add error to y\nsim_data2<-lapply(tmp,function(x){\n  simulate_data(data = x %>% select(-group),y='mpg')\n  \n}) %>%\n  do.call('rbind',.) %>%\n  na.omit() %>%\n  data.frame()\n\n\n# #note see which groups we lost\n# dim(sim_data)\n# dim(mtcars)\n\n\n\n4.7.5 Prediction\nPredict mpg for the simulated data and compare model error.\n\npred1<-predict(mod3,sim_data)\npred2<-predict(smod,sim_data)\n\n#calculate error in original units of y - Root mean squared error (RMSE)\nlibrary(Metrics)\n\ny<-'mpg'\npred1_RMSE<-rmse(sim_data[,y,drop=TRUE],pred1 )\npred2_RMSE<-rmse(sim_data[,y,drop=TRUE],pred2 )\n\n# error in y\ndata.frame(model=c('mod3','smod'),RMSE=c(pred1_RMSE,pred2_RMSE))\n\n  model     RMSE\n1  mod3 3.566637\n2  smod 3.564494\n\n# no error in y\npred1<-predict(mod3,sim_data2)\npred2<-predict(smod,sim_data2)\npred1_RMSE<-rmse(sim_data2[,y,drop=TRUE],pred1 )\npred2_RMSE<-rmse(sim_data2[,y,drop=TRUE],pred2 )\n\ndata.frame(model=c('mod3','smod'),RMSE=c(pred1_RMSE,pred2_RMSE))\n\n  model     RMSE\n1  mod3 3.991189\n2  smod 3.774392\n\n\n\n\n4.7.6 Model diagnostics\nA useful analysis is to visualize the model residuals vs the actual values.\n\nlibrary(ggrepel) # improved text plotting\nresidual<-{sim_data[,y,drop=TRUE]- pred1} %>% # absolute value\n  data.frame(actual=sim_data[,y,drop=TRUE],residual=.,row_name=row.names(sim_data))  \n\nresid_plot<-ggplot(residual, aes(x=actual,y=residual)) +\n  geom_point() +\n  stat_smooth(color='gray') +\n  geom_text_repel(aes(label=row_name))\n\nresid_plot\n\n\n\n\nWe can also plot the predicted vs the actual values for each group of the simulated data.\n\npredicted<-predict(mod3,sim_data)\n\n\n#add group info\ngroup<-c('am','vs','cyl','gear')\ndata<-sim_data %>%\n  unite(., 'group',group,remove=FALSE) %>%\n  mutate(row_name=rownames(sim_data),predicted=predicted )\n\n\npred_plot<-ggplot(data, aes(y = mpg, x =predicted)) +\n  geom_point(aes(color = group), size = 3) +\n  stat_smooth(method = 'lm', color = 'gray') +\n  geom_text_repel(aes(label = row_name, color = group), show.legend = FALSE) +\n  scale_color_discrete(name = paste(group, collapse = '_')) +\n  geom_abline(slope = 1,intercept = 0,linetype='dashed')  #line if predicted and actual perfectly matched\n  \npred_plot\n\n\n\n\nFinally we could compare the density distributions and test for significant differences in model predictions.\n\n#we want to plot the difference from the true vs. predicted value, residual\nresid1<-abs(sim_data[,y,drop=TRUE]-pred1) %>% # absolute value\n  data.frame(model='mod3',residual=.)  \nresid2<-abs(sim_data2[,y,drop=TRUE]-pred2) %>% # absolute value\n  data.frame(model='smod',residual=.)  \n\ndata<-rbind(resid1,resid2)\n\ncols <- c(\"#F76D5E\", \"#72D8FF\") # custom colors as hex codes\n\n# Density areas without lines\ndist_plot<-ggplot(data, aes(x = residual, fill = model)) +\n  geom_density(alpha = 0.8, color = NA) +  # color is the border\n  scale_fill_manual(values = cols) # set custom colors\n\ndist_plot\n\n\n\n\nTest for difference in model errors.\n\nt.test(resid1$residual,resid2$residual) # note we may want to test the shifted log  (i.e. log(x+10)) of the residuals to make them normal or use a non-parametric test\n\n\n    Welch Two Sample t-test\n\ndata:  resid1$residual and resid2$residual\nt = -0.28963, df = 49.539, p-value = 0.7733\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.650378  1.234485\nsample estimates:\nmean of x mean of y \n 2.853965  3.061911 \n\n\nBased on this analysis we can conclude the smaller model (i.e. less terms) is not significantly different from the model with more terms. In practice we want to proceed with interpreting the simplest model with the lowest error."
  },
  {
    "objectID": "ggplot2_intro.html#combining-multiple-plots",
    "href": "ggplot2_intro.html#combining-multiple-plots",
    "title": "4  Plotting for data analysis",
    "section": "4.8 Combining multiple plots",
    "text": "4.8 Combining multiple plots\nThe pathchwork R library makes it easy to combine multiple plots using a variety of custom layouts. Combining plots is as easy as creating individual plots and then defining how they should be combined to create a single visualization based on their layout and position (e.g. in rows and/or columns).\n\nlibrary(patchwork)\n\n\npatchwork <- (dist_plot + resid_plot) / pred_plot\npatchwork + plot_annotation(\n  title = 'Model diagnostic plot',\n  subtitle = 'Comparison of model residual distributions, residuelas and predicted values',\n  caption = 'Example of what is possible'\n)"
  },
  {
    "objectID": "ggplot2_intro.html#appendix",
    "href": "ggplot2_intro.html#appendix",
    "title": "4  Plotting for data analysis",
    "section": "4.9 Appendix",
    "text": "4.9 Appendix\n\nggplot2 cheatsheet\nggplot2 tutorial"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory data analysis (EDA) is helpful for summarizing variables, accessing data quality, exploring multivariate trends and refining data analysis strategies."
  },
  {
    "objectID": "eda.html#data-summary",
    "href": "eda.html#data-summary",
    "title": "5  Exploratory Data Analysis",
    "section": "5.1 Data Summary",
    "text": "5.1 Data Summary\nWhen faced with a new data set, a great first step is to identify the types of variables and summarize them. Lets use this as an opportunity to practice the skills we have learned so far. Later we will use some R libraries to take our data summary skills to a new level.\n\n5.1.1 Overview and summary\n\nlibrary(dplyr)\n\ndata<-mtcars\ndata(data)\n\n#data structure\nstr(data)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n#variable types\nlapply(data,class)  %>%\n  unlist() %>%\n  setNames(.,names(data))\n\n      mpg       cyl      disp        hp      drat        wt      qsec        vs \n\"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n       am      gear      carb \n\"numeric\" \"numeric\" \"numeric\" \n\n#basic summary\nsummary(data)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nNext lets use the skimr library for a quick data overview.\n\n\n5.1.2 skimr\n\nlibrary(skimr)\n\nskim(data) \n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nDo you recognize the output format? This is the same as we previously created in the data wrangling section. Additional information includes overview of missing values, quantiles and variable histograms. This method is specific for different types.\n\nlibrary(dplyr)\nmtcars %>%\n  mutate(cyl=as.factor(cyl)) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncyl\n0\n1\nFALSE\n3\n8: 14, 4: 11, 6: 7\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nNotice how we get a separate summary for each variable type."
  },
  {
    "objectID": "eda.html#summarytools-summary",
    "href": "eda.html#summarytools-summary",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 summarytools summary",
    "text": "5.2 summarytools summary\n\nlibrary(summarytools)\n\n.summary <- dfSummary(data)\n# view(.summary)  #view html report"
  },
  {
    "objectID": "eda.html#data-quality",
    "href": "eda.html#data-quality",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Data Quality",
    "text": "5.3 Data Quality\nData quality assessment is an important first step for any analysis. Ideally the experimental design includes replicated quality control samples which can be used for this purpose. For this demo we will assess variability for a grouping variable.\n\n.summary<-mtcars %>%\n  mutate(cyl=as.factor(cyl)) %>%\n  group_by(cyl) %>%\n  skim()\n\n.summary\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\ncyl\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncyl\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n4\n0\n1\n26.66\n4.51\n21.40\n22.80\n26.00\n30.40\n33.90\n▇▃▂▃▃\n\n\nmpg\n6\n0\n1\n19.74\n1.45\n17.80\n18.65\n19.70\n21.00\n21.40\n▅▂▂▁▇\n\n\nmpg\n8\n0\n1\n15.10\n2.56\n10.40\n14.40\n15.20\n16.25\n19.20\n▂▁▇▃▂\n\n\ndisp\n4\n0\n1\n105.14\n26.87\n71.10\n78.85\n108.00\n120.65\n146.70\n▇▂▂▆▃\n\n\ndisp\n6\n0\n1\n183.31\n41.56\n145.00\n160.00\n167.60\n196.30\n258.00\n▇▁▁▂▂\n\n\ndisp\n8\n0\n1\n353.10\n67.77\n275.80\n301.75\n350.50\n390.00\n472.00\n▇▅▃▂▅\n\n\nhp\n4\n0\n1\n82.64\n20.93\n52.00\n65.50\n91.00\n96.00\n113.00\n▃▆▁▇▃\n\n\nhp\n6\n0\n1\n122.29\n24.26\n105.00\n110.00\n110.00\n123.00\n175.00\n▇▃▁▁▂\n\n\nhp\n8\n0\n1\n209.21\n50.98\n150.00\n176.25\n192.50\n241.25\n335.00\n▇▂▃▁▁\n\n\ndrat\n4\n0\n1\n4.07\n0.37\n3.69\n3.81\n4.08\n4.16\n4.93\n▇▅▃▁▂\n\n\ndrat\n6\n0\n1\n3.59\n0.48\n2.76\n3.35\n3.90\n3.91\n3.92\n▂▂▁▂▇\n\n\ndrat\n8\n0\n1\n3.23\n0.37\n2.76\n3.07\n3.12\n3.22\n4.22\n▃▇▁▁▁\n\n\nwt\n4\n0\n1\n2.29\n0.57\n1.51\n1.89\n2.20\n2.62\n3.19\n▇▅▇▂▅\n\n\nwt\n6\n0\n1\n3.12\n0.36\n2.62\n2.82\n3.21\n3.44\n3.46\n▅▂▁▂▇\n\n\nwt\n8\n0\n1\n4.00\n0.76\n3.17\n3.53\n3.76\n4.01\n5.42\n▇▇▁▁▃\n\n\nqsec\n4\n0\n1\n19.14\n1.68\n16.70\n18.56\n18.90\n19.95\n22.90\n▃▇▇▁▂\n\n\nqsec\n6\n0\n1\n17.98\n1.71\n15.50\n16.74\n18.30\n19.17\n20.22\n▃▇▃▃▇\n\n\nqsec\n8\n0\n1\n16.77\n1.20\n14.50\n16.10\n17.18\n17.56\n18.00\n▂▂▁▅▇\n\n\nvs\n4\n0\n1\n0.91\n0.30\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nvs\n6\n0\n1\n0.57\n0.53\n0.00\n0.00\n1.00\n1.00\n1.00\n▆▁▁▁▇\n\n\nvs\n8\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nam\n4\n0\n1\n0.73\n0.47\n0.00\n0.50\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nam\n6\n0\n1\n0.43\n0.53\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n8\n0\n1\n0.14\n0.36\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ngear\n4\n0\n1\n4.09\n0.54\n3.00\n4.00\n4.00\n4.00\n5.00\n▁▁▇▁▂\n\n\ngear\n6\n0\n1\n3.86\n0.69\n3.00\n3.50\n4.00\n4.00\n5.00\n▃▁▇▁▂\n\n\ngear\n8\n0\n1\n3.29\n0.73\n3.00\n3.00\n3.00\n3.00\n5.00\n▇▁▁▁▁\n\n\ncarb\n4\n0\n1\n1.55\n0.52\n1.00\n1.00\n2.00\n2.00\n2.00\n▇▁▁▁▇\n\n\ncarb\n6\n0\n1\n3.43\n1.81\n1.00\n2.50\n4.00\n4.00\n6.00\n▃▁▇▁▂\n\n\ncarb\n8\n0\n1\n3.50\n1.56\n2.00\n2.25\n3.50\n4.00\n8.00\n▇▇▁▁▁\n\n\n\n\n\nWe used group_by to summarize trends for each level of our grouping variable cyl. Notice how the data is formatted. This is referred to as a melted or long data format. Next, lets calculate the coefficient of variation (CV; std/mean) for each column in our data and level of cyl.\n\n#calculate CV\nCV <-.summary %>%\n select(contains(c('mean','sd'))) %>%\n  {(.[2]/.[1]) * 100} %>%\n  setNames(.,'CV')\n\n.summary['CV']<- CV\n\nPlot the CV vs. the mean for each variable separately for each level of cyl.\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\ntheme_set(theme_minimal()) # set theme globaly\noptions(repr.plot.width = 2, repr.plot.height =3) # globaly set wi\n\nggplot(.summary, aes(x=numeric.mean,y=CV,color=cyl)) + \n  geom_point(size=2, alpha=.75) +\n  geom_text_repel(aes(label=skim_variable),show.legend = FALSE) +\n  facet_grid(.~cyl) +\n  scale_color_brewer(palette = 'Set1') +\n  xlab('mean') +\n  ylab('Coefficient of variation')\n\n\n\n\nThis plot is useful to identify variables with low precision which may need to be omitted for further analyses. In the case of mtcars we see the variables with high CV compared to their mean should all be categorical. For example we can check that am shows a large differences for levels of cyl.\n\nmtcars %>%\n  select(one_of(c('cyl','am'))) %>%\n  table()\n\n   am\ncyl  0  1\n  4  3  8\n  6  4  3\n  8 12  2"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Multivariate Analysis",
    "text": "5.4 Multivariate Analysis\nNext lets visualize sample (row) trends given all variables (columns) using principal components analysis (PCA). First, lets calculate the principal components and visualize their variance explained.\n\n#calculate and show eigenvalue summary\npca_obj <- prcomp(data,scale= TRUE)\n\n\n5.4.1 Visualize optimal principal components (PCs) to retain.\n\n#notice the summary method does not return the results as printed.\n#we could modify the method todo so or replicate results\neigenvals<-pca_obj$sdev\neigenvals_cumsum<-cumsum(eigenvals)\nvar_explained<-sum(eigenvals)\nprop_var_exp<-eigenvals/var_explained\nprop_cumsum<-eigenvals_cumsum/var_explained\n\npca_eigen<-data.frame(PC=1:length(eigenvals),var_explained=prop_var_exp,eigen_cumsum=prop_cumsum)\n\nPlot eigenvalues to select total number of PCs\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\ndf<-melt(pca_eigen,id.vars = 'PC')\n\nggplot(df, aes(\n  x = as.factor(PC),\n  y = value,\n  fill = variable,\n  group = variable\n)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  scale_fill_brewer(\n    palette = 'Set1',\n    labels = c('variance explained', 'cummulative variance\\nexplained')\n  ) +\n  theme_minimal() +\n  geom_hline(yintercept = .8, linetype = \"dashed\") +\n  xlab('Principal Components')"
  },
  {
    "objectID": "eda.html#create-a-scatter-plot-matrix-to-visualize-pc-scores",
    "href": "eda.html#create-a-scatter-plot-matrix-to-visualize-pc-scores",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Create a scatter plot matrix to visualize PC scores",
    "text": "5.5 Create a scatter plot matrix to visualize PC scores\nVisualize each pairwise PC comparison and color sample scores by cyl.\n\nlibrary(GGally)\nlibrary(plotly)\n\nlimit<-5\ngroup<-'cyl'\n\ndf <- pca_obj$x[,1:limit] %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) %>%\n  left_join(mtcars %>% select(one_of(group)) %>% mutate(id = rownames(.))) %>%\n  select(-id) %>%\n  mutate(group=as.factor(!!sym(group)))\n\n  \np <- ggpairs(df, ggplot2::aes(colour=group ))\n\nggplotly(p)\n\n\n\n\n\n\n5.5.1 Visualize specific PC sample scores\nPlot the principal plane (PC1 vs PC2) and identify scores for different numbers of cyl with ellipses.\n\nlibrary(ggrepel)\nscores_df <- pca_obj$x %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) %>%\n  left_join(mtcars %>% mutate(id = rownames(.), cyl = as.factor(cyl)))\n\nggplot(scores_df, aes(x = PC1, y = PC2, color = cyl)) +\n  geom_point(size=3) +\n  geom_text_repel(aes(label=id), show.legend = FALSE, max.overlaps = nrow(df),force=100) +\n  stat_ellipse(aes(fill=cyl), geom = \"polygon\",alpha=.25, show.legend = FALSE) +\n  ggtitle('Scores')\n\n\n\n\nThis visualization is helpful for identify similarity within and between different numbers of cyl. For example, we can see that the biggest differences (in variables) are between cyl=4 and cyl=8. If we expect sample scores to be bivariate normal in the scores space, samples outside the ellipses can be helpful for identify moderate outliers among groups of cyl.\n\n\n5.5.2 Visualize variable loadings\n\ndf <- pca_obj$rotation %>%\n  as.data.frame() %>%\n  mutate(id = rownames(.)) \n\nggplot(df, aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_text_repel(aes(label=id), show.legend = FALSE) +\n  ggtitle('Loadings')\n\n\n\n\n\n\n5.5.3 Create a custom component for axis labels\nWe will show the percent explained for each PC.\n\nprcomp_axis_label <- function(obj,digits=1) {\n  \n  n<-obj %>% length()\n  .name<-\n    paste0(\n    rep('PC',n),\n    c(1:n)\n  )\n  \n  paste0(.name,'[',round(obj,digits),'%]') %>%\n    setNames(.name)\n\n}\n\nprcomp_axis_label(pca_eigen$var_explained*100,0)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n\"PC1[33%]\" \"PC2[21%]\" \"PC3[10%]\"  \"PC4[7%]\"  \"PC5[6%]\"  \"PC6[6%]\"  \"PC7[5%]\" \n       PC8        PC9       PC10       PC11 \n \"PC8[4%]\"  \"PC9[4%]\" \"PC10[3%]\" \"PC11[2%]\" \n\n\n\nmy_labels<-prcomp_axis_label(pca_eigen$var_explained*100,0)\nx <-'PC1'\ny<-'PC4'\n\n.df<-df %>%\n  select(one_of(c(x,y,'id')))\n\nggplot(.df, aes_string(x = x, y = y)) +\n  geom_point() +\n  geom_text_repel(aes(label=id), show.legend = FALSE, max.overlaps = nrow(df),force=3) +\n  ggtitle('Variable loadings') +\n  ylab(my_labels[y]) +\n  xlab(my_labels[x])\n\n\n\n\nLoadings can be used to identify which variables are most different between groups of sample scores. For example, since groups of cyl scores are spread in the x-axis (PC1) we can look at the variables with largest loadings on PC1 (largest negative and positive PC1 values (position on the x-axis) to identify the largest differences in variables between groups of cyl. This can be useful to identify that cars with smaller number of cyl have higher mpg and lower disp.\nWe can investigate this observation by making a custom visualization.\n\np<-ggplot(mtcars, aes(x = disp, y = mpg,color=as.factor(cyl))) +\n  geom_point(size=3)\n\np\n\n\n\n\nNotice how we can almost perfectly separate groups of cyl based on these two dimensions?\n\nlibrary(tidyr)\n#calculate min and max for variables given groups of cyl\n.ranges<-mtcars %>% \n  group_by(cyl) %>% \n  summarise(min_mpg = min(mpg), max_mpg = max(mpg),\n            min_disp = min(disp), max_disp = max(disp)) %>%\n  gather(variable, value, -cyl) %>% \n  separate(variable, into = c(\"variable\", \"stat\"), sep = \"_\") %>% \n  spread(stat, value)\n\n\n#add rectangles to the plot\n#note: this is sub optimal as we need to know what is x or y axis in the plot\ntmp<- .ranges %>%\n  split(., .$cyl)\n\n#need to know colors to set rectangle colors\np<-p + \n  scale_colour_brewer(\n  palette = 'Set1',\n  aesthetics = \"colour\"\n)\n#get color codes\nlibrary(RColorBrewer)\n.colors<-brewer.pal(length(levels(mtcars$cyl)), 'Set1')\n\nfor(x in 1:length(tmp)){\n  \n  i<-tmp[[x]]\n  xmin<- i %>%\n    filter(variable == 'min') %>%\n    select(disp) %>% \n    .[1,,drop=TRUE]\n  xmax<- i %>%\n    filter(variable == 'max') %>%\n    select(disp) %>% \n    .[1,,drop=TRUE]\n  ymin<- i %>%\n    filter(variable == 'min') %>%\n    select(mpg) %>% \n    .[1,,drop=TRUE]\n  ymax<- i %>%\n    filter(variable == 'max') %>%\n    select(mpg) %>% \n    .[1,,drop=TRUE]\n    \np<-p +\n  annotate(\"rect\", xmin = xmin , xmax = xmax, ymin = ymin, ymax = ymax,\n           alpha = .5,fill = .colors[x])\n  \n}\n\n\np + guides(color=guide_legend(title=\"number of cylinders\"))"
  },
  {
    "objectID": "eda.html#appendix",
    "href": "eda.html#appendix",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Appendix",
    "text": "5.6 Appendix\n\nsummarytools\nskimr"
  },
  {
    "objectID": "reproducible_reports.html",
    "href": "reproducible_reports.html",
    "title": "6  Reproducible reports",
    "section": "",
    "text": "DALL·E 2023-04-12 21.10.39 - duck creating a reproducible report using the R programming language, colorful cartoon, solarpunk\n\n\n\n\n\n\n\n\n All of the examples you have seen so far have used rmarkdown and quarto to combine text and R code into reproducible documents. This powerful combination offers many options to combine code and text, which can be rendered in different formats and typeset in custom layouts.\nIf you look at the source code of the examples you can see how markdown (text) and R code blocks have been combined. Next we will look at more examples of how we can seamlessly combine text and code, recreate data analysis results in different formats and create automated and parameterized reports.\n\n\nLets create some data objects in R and incorporate them directly into the text results.\n\n\n\nThe mtcars data set contains 11 measurements for 32 samples.\nIn this example we have hidden the code which was used to create the text.\ncode\n\ntext\n\nWe have also used css to style the the image of the results (added a border).\nWe can use R studio UI to switch between different representations the code and rendered the final results in different formats. Change between source and visual representations in the top left corner.\n\nThe visual editor can also be used to create many markdown scaffolds to help with the type setting and layout.\nWe can configure our report’s front matter to define default options for a variety of output formats.\n\nTheir are many options we can specify for our report using the front matter code block at the beginning of our document which is designated using a modified yaml format.\nFor example, try adding the following block with your custom definitions and render the report.\n    ---\n    title: Reproducible reports\n    author: Best ever!\n    date: \"04/1/2023\"\n    ---\nWe can also execute custom R code in the front matter.\n    ---\n    title: Reproducible reports\n    author: Your name goes here\n    date: \"April 25, 2023\"\n    ---\nThe general format is as follows.\n    ---\n    key1: key1_value1\n      key1_child1: key1_child1_value\n    key2: key2_value1\n    ---\nCheck out the quarto gallery for more inspirational examples.\n\n\n\nWe can specify options for our code in the parameters block in the front matter. For example, we can optionally create a custom code and text block summary. We will add the params option to our document’s front matter.\nThe following sets the variable named summarize_columns to TRUE when we execute R code blocks, which we can use to dynamically modify our report.\nparams:\n  summarize_columns: TRUE\nNow we can execute custom logic in our report’s code blocks based on the values in the params variable.\nif(exists('params') && params$summarize_columns){\n  .text<-paste(colnames(get(data_name)),collapse=', ')\n  .text<-paste('The data contains the following variables:\\n',.text)\n} else {\n  \n  .text<-tags$p('\\U2717 Hello world, not unlocked.',style =\"color: red;\") %>%\n    as.character()\n  #Unicode Characters: https://www.w3schools.com/charsets/ref_utf_dingbats.asp\n}\n\nHTML(.text)\n\nThe data contains the following variables:\n mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nParams are useful to create custom reports. For example we can supply the parameters when rendering using command line arguments (run the flowing in the terminal).\nquarto render reproducible_reports.qmd -P summarize_columns:FALSE\nTo make this more interesting, lets make the data a changeable parameter.\nparams:\n  summarize_columns: TRUE\n  report_data: 'mtcars'\nWe also need to modify how the data is scoped in our code on line 25.\ndata_name<-ifelse(is.null(params$report_data),get('mtcars'),get(params$report_data))\nNow let’s recreate the report for another data set.\nquarto render reproducible_reports.qmd -P report_data:'iris'\nIn practice this example could define the file path for the custom data which needs to be loaded and analyzed.\n\n\n\nThere are many options for creating custom tables in rmarkdown. For example: knitr, formattable, DataTable and gtextras.\nThe choice of library depends on the output format and desired customization. The following are a few examples of what is possible.\n\n\n\n.data<-head(data)\nknitr::kable(.data, caption = \"knitr::kable\")\n\n\nknitr::kable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\nlibrary(formattable)\n\n#create custom formats for column\nformats <- list(color_tile(\"transparent\", \"lightpink\"),color_bar(\"lightgreen\")) %>%\n  setNames(.,colnames(data)[1:2])\n\ntable1<-formattable(.data, formats,caption ='formattable')\ntable1\n\n\n\n\nformattable\n\n\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\n\n\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225\n\n\n105\n\n\n2.76\n\n\n3.460\n\n\n20.22\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\nlibrary(DT)\n\ndatatable(.data, filter = 'top', options = list(\n  pageLength = 5, autoWidth = TRUE\n))\n\n\n\n\n\n\nNote we can also convert a formattable to and interactive DT table using `as.datatable(<formattable>)`.\n\nas.datatable(table1,filter = 'top', options = list(\n  pageLength = 5, autoWidth = TRUE\n))\n\n\n\n\n\n\n\n\n\n\nThe gtExtras package can be used to include custom ggplot2 base visualizations right inside your tables.\n\n\n\n\nQuarto offers many layout options. The simplest control is based on the number of columns or rows we want the code results to fill. For example to create two figures side by side we can use the following.\n::: {layout-ncol=2}\n<CODE>\n:::\n\n\n\nplot(data[,1])\n\n\n\n\n\nplot(data[,2])\n\n\n\n\n\n\nWhen using the html format for the report we have many other options like creating tabsets.\n\nPlot 1Plot 2\n\n\n\nplot(data[,1])\n\n\n\n\n\n\n\nplot(data[,2])\n\n\n\n\n\n\n\n\n\n\nNext let’s add references to our report. We can specify the bibliography in the front matter as follows. Note this variable can also be set for all documents in the _quarto.yml.\nbibliography: references.bib\nThe .bib BibTex format can be created using various tools and looked up for any R library using citation('library'). For example lets add a citation for the R base library to our report.\n\ncitation('base')\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nWe can add the following entry to references.bib.\n@Manual{R,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\nNow we can cite R using the following syntax [@citation]. We need to add an additional code block to define where the references should be rendered.\n## References\n\n::: {#refs}\n:::\nThe following report was created using (R Core Team 2022).\n\n\n\n\nGetting started with quarto\n\n\n\n\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/."
  },
  {
    "objectID": "data_types.html",
    "href": "data_types.html",
    "title": "2  Data types",
    "section": "",
    "text": "R uses different types to describe individual units of data, datum, and stores these in different types of data objects. The following examples are meant to get you up and running with common R data types and data.frames. Take a look in the appendix for more comprehensive examples.\nThe major types of datum include.\nIndividual datum can be combined in the following common data structures:\nNote, many advanced data types exist which are optimized for specific use cases e.g. arrays and tibbles."
  },
  {
    "objectID": "data_types.html#hands-on",
    "href": "data_types.html#hands-on",
    "title": "2  Data types",
    "section": "2.1 Hands on",
    "text": "2.1 Hands on\nLets create and manipulate some datum\n\n2.1.1 Making datum\nLogical vectors can be used to subset data, execute logical comparisons (e.g. and, or and not) and are interpreted numerically as TRUE = 1 and FALSE = 0. Lets create a logical datum and assign (i.e. save) it to the name obj.\n\nobj<-TRUE\nstr(obj)\n\n logi TRUE\n\n\n\n\n2.1.2 Introspection\nUsing the str function is a great way to inspect the properties of a datum and data type. Other useful introspection functions include:\n\nlength - length\ndim - dimensions (e.g. number of rows and columns)\nclass - object-oriented class\ntypeof - type of the datum\nsummary - numeric summary\nnames - object names used for subsetting\n\n\n\n2.1.3 Combining datum\nLets add some more datum elements to our obj.\n\nobj<-c(obj,0)\nobj\n\n[1] 1 0\n\n\nThis action took advantage of the numeric translation of logicals (see above) and used c to add another element. R also coerced (i.e. changed) the data types in the process to numeric.\n\nstr(obj)\n\n num [1:2] 1 0\n\n\nWe can convert types. For example lets convert the binary (i.e two values) obj back to a logical vector (i.e. object with a one-dimension (i.e. length) and one type).\n\nas.logical(obj)\n\n[1]  TRUE FALSE\n\n\n\n\n2.1.4 Data types\nUsing c we already created a vector data type. Next lets create a character vector and explore how logical vectors can be used to subset (i.e. select) specific datum.\n\npet<-c('dog','parrot','chicken','duck')\nbird<-c(FALSE,TRUE,TRUE,TRUE)\npet[bird]\n\n[1] \"parrot\"  \"chicken\" \"duck\"   \n\n\nDoing this we have used the [] function to extract elements from the the object pets based on the logical vector object bird. This returns all values equal to TRUE in bird. Logical vectors are commonly used for comparisons (e.g. logical operations).\nFor example, we get can easily get all pets which are not birds. This takes advantage of conditional subsetting.\n\npet[!bird]\n\n[1] \"dog\"\n\n\nThe example above first calculates all objects not (!) equal to bird == TRUE then extracts all TRUE values from the object pets (rows).\nWe can use logical operations using multiple logical vectors. For example, lets define if an animals is friendly and select all friendly bird(s).\n\nfriendly<-c(TRUE,TRUE,TRUE,FALSE)# define characteristics of each pet\npet[friendly & bird]\n\n[1] \"parrot\"  \"chicken\"\n\n\n\n\n2.1.5 Combining datum in data objects\nThe examples above created vectors for the various combinations of datum. A vector is Rs default method to store one-dimensional (i.e. only have length) combinations of objects of the same type. Next lets store multiple vectors of different types using a data.frame.\n\n(df<-data.frame(pet=pet,bird=bird,friendly=friendly)) # note we can place the assignment '<-' in parentheses '()' to print the results\n\n      pet  bird friendly\n1     dog FALSE     TRUE\n2  parrot  TRUE     TRUE\n3 chicken  TRUE     TRUE\n4    duck  TRUE    FALSE\n\n\nWe can check the dimensions, and column and row names of data.frames as follows.\n\ndim(df)  \n\n[1] 4 3\n\ncolnames(df)\n\n[1] \"pet\"      \"bird\"     \"friendly\"\n\nrownames(df)\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\n#note we can get both and column and row names using 'dimnames'\n\nWhen we created the data frame we nameed the elements, which allows us to subset them using the $ notation. For example, we can do the following to get all bird elements which are not friendly.\n\ndf[df$bird & !df$friendly,]\n\n   pet bird friendly\n4 duck TRUE    FALSE\n\n\nNotice, we can index a two-dimensional data.frame using the same [] operator and specify rows or columns using the notation [rows,columns]. The example above returned all rows which are bird = TRUE and not friendly = TRUE and return all columns for the results.\nIf we wanted to only know the pet which is a bird and is not friendly we can do as follows.\n\ndf[df$bird & !df$friendly,]$pet\n\n[1] \"duck\"\n\n\nA common data analysis task is to ask if specific elements are %in% (in) the elements of a data object. For example, we can get all birds which are not equal to pet ='duck'.\n\ndf[!df$pet %in% 'duck' & df$bird,]\n\n      pet bird friendly\n2  parrot TRUE     TRUE\n3 chicken TRUE     TRUE\n\n\nNote, we could have done this is in a more programmatic manner by operating on objects created from the individual calculations. This makes it easier to read your code and simplifies recalculations give changes in inputs.\n\nnot_friendly_bird<-df[df$bird & !df$friendly,]$pet\n\ndf[!df$pet %in% not_friendly_bird & df$bird,]\n\n      pet bird friendly\n2  parrot TRUE     TRUE\n3 chicken TRUE     TRUE\n\n\n\n\n2.1.6 Missing values\nMissing values are denoted in R as NA. Note, other special definitions include Inf,-Inf, NaN and NULL, interpreted as positive and negative infinity, not a number and undefined, respectively. Missing values need to be omitted, imputed or handled in functions else these can cause errors or will be propagated as NA.\nNext lets add missing values to or data and retry some of the examples above to see what happens. To create missing values we will use a logical operation to select a specific row, select a column for that row and then assign to NA.\n\ndf[df$pet == 'dog',]$bird<-NA\ndf\n\n      pet bird friendly\n1     dog   NA     TRUE\n2  parrot TRUE     TRUE\n3 chicken TRUE     TRUE\n4    duck TRUE    FALSE\n\n\nLets see what happens when we try select all friendly birds.\n\nnot_friendly_bird<-df[df$bird & !df$friendly,]$pet\n\ndf[!df$pet %in% not_friendly_bird & df$bird,]\n\n       pet bird friendly\nNA    <NA>   NA       NA\n2   parrot TRUE     TRUE\n3  chicken TRUE     TRUE\n\n\nNotice the NA is propagated in the results which can later cause errors.\nWe can omit the NA either before (recommended) or after the calculation.\n\noriginal_df<-df # save original data\ndf<-na.omit(df) # remove all columns and/or rows with a missing value\n\n#notice we want to recalculate the original logical operators for the new data since it changed shape\nnot_friendly_bird<-df[df$bird & !df$friendly,]$pet\ndf[!df$pet %in% not_friendly_bird & df$bird,]\n\n      pet bird friendly\n2  parrot TRUE     TRUE\n3 chicken TRUE     TRUE\n\n\nWe can also check if a row or a column has an NA and treated in a custom manner. For example we can fix it.\n\ndf<-original_df\n\ndf[is.na(df$bird),]$bird<-FALSE\ndf\n\n      pet  bird friendly\n1     dog FALSE     TRUE\n2  parrot  TRUE     TRUE\n3 chicken  TRUE     TRUE\n4    duck  TRUE    FALSE\n\n\nWe can do as follow to remove any columns with all values == NA.\n\ndf<-original_df\ndf$bad<-NA\nall_missing_columns<-colSums(is.na(df)) == nrow(df)\n\ndf[,!all_missing_columns]\n\n      pet bird friendly\n1     dog   NA     TRUE\n2  parrot TRUE     TRUE\n3 chicken TRUE     TRUE\n4    duck TRUE    FALSE"
  },
  {
    "objectID": "data_types.html#lists",
    "href": "data_types.html#lists",
    "title": "2  Data types",
    "section": "2.2 Lists",
    "text": "2.2 Lists\nAfter data.frames, lists are the most commonly used data types in R. A list can be used to store different types of datum and of unequal lengths. We will learn more about lists later. For now lets compare lists to data.frames.\nLets create a list and convert a convert a data.frame to a list.\n\n(df_list<- list(pet=pet,bird = bird,friendly = friendly)) # name = value\n\n$pet\n[1] \"dog\"     \"parrot\"  \"chicken\" \"duck\"   \n\n$bird\n[1] FALSE  TRUE  TRUE  TRUE\n\n$friendly\n[1]  TRUE  TRUE  TRUE FALSE\n\n(df_list2<-as.list(df)) # we can also convert a data.frame to a list\n\n$pet\n[1] \"dog\"     \"parrot\"  \"chicken\" \"duck\"   \n\n$bird\n[1]   NA TRUE TRUE TRUE\n\n$friendly\n[1]  TRUE  TRUE  TRUE FALSE\n\n$bad\n[1] NA NA NA NA\n\nidentical(df_list,df_list2)\n\n[1] FALSE\n\n\nSimilar to data.frames we can extract list elements based on their name.\n\ndf_list$pet\n\n[1] \"dog\"     \"parrot\"  \"chicken\" \"duck\"   \n\n\nWe can also get items based on their numeric index (order in list).\n\ndf_list[1] # this returns the list element name and values\n\n$pet\n[1] \"dog\"     \"parrot\"  \"chicken\" \"duck\"   \n\ndf_list[[1]] # this returns only the values\n\n[1] \"dog\"     \"parrot\"  \"chicken\" \"duck\"   \n\n\nWe can unpack all the elements in the list and return a vector.\n\nunlist(df_list)  #notice mixed types are converted to strings (i.e. quoted text)\n\n     pet1      pet2      pet3      pet4     bird1     bird2     bird3     bird4 \n    \"dog\"  \"parrot\" \"chicken\"    \"duck\"   \"FALSE\"    \"TRUE\"    \"TRUE\"    \"TRUE\" \nfriendly1 friendly2 friendly3 friendly4 \n   \"TRUE\"    \"TRUE\"    \"TRUE\"   \"FALSE\""
  },
  {
    "objectID": "data_types.html#matrices-and-arrays",
    "href": "data_types.html#matrices-and-arrays",
    "title": "2  Data types",
    "section": "2.3 Matrices and Arrays",
    "text": "2.3 Matrices and Arrays\nMatrices (two dimensional table) and arrays (n dimensional table) are often used for specialized mathematical calculations. Matrices can be useful for organizing vectors into different dimensions.\nLets represent a vector as table with custom number of rows and columns.\n\ntmp<-unlist(df_list)\n(mat<-matrix(tmp,ncol=3)) # note this fills by columns\n\n     [,1]      [,2]    [,3]   \n[1,] \"dog\"     \"FALSE\" \"TRUE\" \n[2,] \"parrot\"  \"TRUE\"  \"TRUE\" \n[3,] \"chicken\" \"TRUE\"  \"TRUE\" \n[4,] \"duck\"    \"TRUE\"  \"FALSE\"\n\n# matrix(tmp,ncol=3,byrow = TRUE)  #fill by row\n\nMatrices are useful for many other purposes. Unlike data.frames they do not store mixed types (i.e. when numeric and other types are mixed all values are coerced to strings).\nWe can convert a matrix to a data.frame.\n\n(df2<-as.data.frame(mat))\n\n       V1    V2    V3\n1     dog FALSE  TRUE\n2  parrot  TRUE  TRUE\n3 chicken  TRUE  TRUE\n4    duck  TRUE FALSE\n\ndimnames(df2)<-dimnames(original_df)# set dimension names\nstr(df2) # notice our original types may have not been preserved\n\n'data.frame':   4 obs. of  3 variables:\n $ pet     : chr  \"dog\" \"parrot\" \"chicken\" \"duck\"\n $ bird    : chr  \"FALSE\" \"TRUE\" \"TRUE\" \"TRUE\"\n $ friendly: chr  \"TRUE\" \"TRUE\" \"TRUE\" \"FALSE\"\n\n\nLastly, we can compare R objects.\n\nidentical(df,df2) # more advanced methods can show what is different\n\n[1] FALSE"
  },
  {
    "objectID": "data_types.html#appendix",
    "href": "data_types.html#appendix",
    "title": "2  Data types",
    "section": "2.4 Appendix",
    "text": "2.4 Appendix\n\nData Carpentry introduction to R\nData types overview"
  }
]